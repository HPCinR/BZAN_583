---
title: "Distributed Computing"
format: revealjs
filters: 
   - include-code-files
---

## Distributed Memory Computing


## Running MPI on a Laptop {.smaller}

macOS in a Terminal window:

* `brew install openmpi`
* In an R session: `install.packages("pbdMPI")`
* `mpirun -np 4 Rscript your_spmd_code.R`

Windows

* Web Page: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi
* Download: https://www.microsoft.com/en-us/download/details.aspx?id=100593
* `pbdMPI` has a Windows binary on CRAN

## Revisit `hello_balance.R`

```{.r include="code/hello_balance.R"}
```

## Working with a remote cluster using R

![](/pics/01-intro/Workflow.png)

## Running Distributed on a Cluster {background-image=/pics/01-intro/WorkflowRunning.png background-size="200px"}

![](/pics/01-intro/BatchRonCluster.jpg)

## Distributed Memory Tools {background-image=/pics/ParallelSoftware/Slide7distributed.jpg background-size="1200px"}

## Message Passing Interface (MPI) {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="1200px"}

## Single Program Multiple Data (SPMD) {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right" .smaller}

* N instances of the same code cooperate
  * Each of the N instances has `rank`, {0, . . ., N-1}
  * The `rank` determines any differences in work
  * Instances run asynchronously 

* SPMD parallelization is a generalization of the serial code
  * Many rank-aware operations are automated
  * Collective operations are high level and easy to learn
  * Explicit point-to-point communications are an advanced topic
  * Multilevel parallelism is possible

* Typically no manager, it is all cooperation
  
## pbdR Project {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right" .smaller}

![](/pics/01-intro/pbdRlib.jpg)

* Bridge HPC with high-productivity of R: Expressive for data and modern statistics

* Keep syntax identical to R, when possible

* Software reuse philosophy:

   * Don't reinvent the wheel when possible
   * Introduce HPC standards with R flavor
   * Use scalable HPC libraries with R convenience

* Simplify and use R intelligence where possible

::: notes
Using HPC concepts and libraries 
* Benefits the R user by knowing standard components of HPC
:::

## {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right" .smaller}

![](/pics/01-intro/pbdRlib.jpg)

## Package `pbdMPI` {.smaller}

* Specializes in SPMD programming for HPC clusters
   * Manages printing from ranks
   * Provides chunking options
   * Provides communicator splits for multilevel parallelism
   * In situ capability to process data from other MPI codes without copy 

* A derivation and rethinking of the `Rmpi` package aimed at HPC clusters

   * Simplified interface with fewer parameters (using R's S4 methods)
   * Faster for matrix and array data - no serialization


::: notes
* Prefer pbdMPI over Rmpi due to simplification and speed
   * No serialization for arrays and vectors
* Drops spawning a cluster
  * Because a client-server relationship is more appropriate
:::

## `pbdMPI`: High-level Collective Communications {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px"  background-position="bottom right" .smaller}

Each of these operations is performed across a `communicator` of ranks. Simplest one is all ranks but rank arrays can be used for multilevel collectives.

* **`reduce()`** Reduces a set of same-size distributed vectors or arrays with an operation (+ is default). Fast because both communication and reduction are parallel and no serialization is needed.

* **`allreduce()`** Same as `reduce()` except all ranks in a `comm` get the result

* **`gather()`** Gathers a set of distributed objects 

* **`allgather()`** Same as `gather()` except all ranks in a `comm` get the result

* **`bcast()`** Broadcasts an object from one rank to all in its `comm`

* **`scatter()`** Broadcasts different pieces of an object from one rank to all in its `comm`

* **`barrier()`** Waits on all ranks in a `comm` before proceeding 


## `pbdMPI`: High-level Collective Operations {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right"}
$\small \bf A = \sum_{i=1}^nX_i$ $\quad$ $\qquad$ $\qquad$ **`A = reduce(X)`** $\qquad$ $\qquad$ **`A = allreduce(X)`**  

$\small \bf A = \left[ X_1 | X_2 | \cdots | X_n \right]$ $\qquad$ **`A = gather(X)`** $\qquad$ $\qquad$ **`A = allgather(X)`**

![](/pics/01-intro/RHistory3sub.png)

::: notes
* Powerful: communication and reduction is highly parallel
   * that's why it beats Spark/MapReduce
:::

## `pbdMPI`: Functions to Facilitate SPMD Programming {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right" .smaller}

* **`comm.chunk()`** splits a number into chunks in various ways and various formats. Tailored for SPMD programming, returning rank-specific results.

* **`comm.set.seed()`** sets the seed of a parallel RNG. If diff = FALSE, then all ranks generate the same stream. Otherwise, ranks generate different streams.

* **`comm.print()`** and **`comm.cat()`** print by default from rank 0 only, with options to print from any or all ranks.


## Distributed Programming Works in Shared Memory {background-image=/pics/ParallelSoftware/Slide6.png background-size="1100px"}


## Hands on Session 5: Hello MPI Ranks {/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right"}

`code_5/hello_world.R`
```{.r include = "code/hello_world.R"}
```

### **Rank** distinguishes the parallel copies of the same code

## Hands on Session 5: Random Forest with MPI {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="bottom right"}

`code_5/rf_mpi.R`
```{.r include="code/rf_mpi.R"}
```

## Hands on Session 5: `comm.chunk()`

`mpi_shorts/chunk.r`
```{.r include="code/chunk.r"}
```

## Hands on Session 5: other short MPI codes {.smaller}

bcast.r  chunk.r comm_split.R  cov.r  gather-named.r  gather.r  gather-unequal.r  hello-p.r  hello.r  map-reduce.r  mcsim.r  ols.r  qr-cop.r  rank.r  reduce-mat.r  timer.r

* These short codes only use `pbdMPI` and can run on a laptop in a terminal window if you installed OpenMPI
* On the clusters these can run on a login node with a small $^*$ number of ranks
* Wile in the `mpi_shorts` directory, run the following
   * `source ../code_4/modules_MACHINE.sh`
   * `mpirun -np 4 Rscript your_script.r`

.footnote[
$^*$ Note that running long or large jobs on login nodes is strongly discouraged
]

## Shared Memory - MPI or fork? {background-image="/pics/01-intro/pbdRlib.jpg" background-size="200px"  background-position="bottom right" .smaller}

::: {layout="[2,1]"}
*  fork via `mclapply()` + `do.call()`  
![](/pics/mpi/fork.jpg){width=70}  
* MPI replicated data + `allreduce()`  
![](/pics/mpi/mpi-replicate.png){width=380}  
* MPI chunked data + `allreduce()`  
![](/pics/mpi/mpi-partition.png){width=380}  


`do.call()` is serial  
`allreduce()` is parallel  
:::

## Package `pbdDMAT` {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px"  background-position="top right" .smaller}

* ScaLAPACK: Distributed version of LAPACK (uses PBLAS/BLAS but not LAPACK)

   * 2d Block-Cyclic data layout - mostly automated in `pbdDMAT` package
   
   * BLACS: Communication collectives for distributed matrix computation
   * PBLAS: Distributed BLAS (uses standard BLAS within blocks)

* R code is identical for most matrix operations by overloading operators and `ddmatrix` class

![](/pics/01-intro/pbdRlib.jpg) {size=300 position="left"}

## Package `pbdML` {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="top right"}

* A demonstration of `pbdDMAT` package capabilities

* Includes
  * Randomized SVD
  * Randomized principal components analysis
  * Robust Principal Component Analysis?" 
    from https://arxiv.org/pdf/0912.3599.pdf

## Hands on Session $\quad$ rsvd:
#### Singular value decomposition via randomized sketching
<br>
Randomized sketching produces fast new alternatives to classical numerical linear algebra computations. 

<br>
Guarantees are given with probability statements instead of classical error analysis.

<br>
Martinsson, P., & Tropp, J. (2020). Randomized numerical linear algebra: Foundations and algorithms. Acta Numerica, 29, 403-572. [https://doi.org/10.48550/arXiv.2002.01387](https://doi.org/10.48550/arXiv.2002.01387)

## Hands on Session $\quad$ rsvd: {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="top right" .smaller}

#### Randomized SVD via subspace embedding
Given an $n\times p$ matrix $X$ and $k = r + 10$, where $r$ is the *effective rank* of $X$:  
1. Construct a $p\times k$ random matrix $\Omega$  
2. Form $Y = X \Omega$  
3. Decompose $Y = QR$  

$Q$ is an orthogonal basis for the columnspace of $Y$, which with high probability is the columnspace of $X$. To get the SVD of $X$:  
1. Compute $C= Q^TX$  
2. Decompose $C = \hat{U}\Sigma V^T$  
3. Compute $U = Q\hat{U}$  
4. Truncate factorization to $r$ columns  

## MNIST Data {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="top right"}

![](/pics/mnist/Rplots.png){size=500 position="left"}

## `mnist_rsvd.R`  
```{.r include="code/mnist_rsvd.R"}
```

## Hands-on Session  rsvd  
```{.r include="code/rsvd_snippet.donotrun"}
```

## Package `kazaam` {background-image=/pics/ParallelSoftware/Slide7mpi.jpg background-size="200px" background-position="top right"}

![](/pics/01-intro/pbdRlib.jpg) {size=300 position="left"}

* Distributed methods for tall matrices (and some for wide matrices) that exploit the short dimension for speed and long dimension for parallelism

* Tall matrices, `shaq` class, are chunked by blocks of rows
* Wide matrices, `tshaq` class, are chunked by blocks of columns

* Much like `pbdDMAT`, most matrix operations in R code are identical to serial through overloading operators and `shaq` `S4` class

.footnote[
Naming is a "tongue-in-cheek" play on 'Shaquille' 'ONeal' ('Shaq') and the film 'Kazaam'
]

## Hands on Session `kazaam`

