---
title: "Measurement and terminology of parallel speedup"
format: 
  revealjs:
    slide-number: c/t
    width: 1200
    height: 800
    chalkboard: true
    code-link: true
    code-line-numbers: false
editor: source
engine: knitr
---

## Embarrassingly (Pleasingly) Parallel {background-image="/pics/parallel/Parallel1.png" background-size="100%" background-position="center" .smaller}  

$$\qquad$$  
- $$t_p = \frac{t}{n} = t_n$$
- $$\mbox{Speedup:}\quad\frac{t}{t_p} = n$$
$t$ - Serial time  
$n$ - Number of chunks (or processes)  
$t_n$ - Single chunk time with $n$ chunks   
$t_p$ - Parallel time  

#### Best case scenario: no overhead, no communication  

## Serial Section (Amdahl's Law) {background-image="/pics/parallel/Parallel2.png" background-size="100%" background-position="center" .smaller}  

$$\qquad$$
$$\qquad$$  
$$t_p = t_s + t_n > t_s$$  
$$\mbox{Maximum Speedup:}\quad\lim_{n \to \infty}\frac{t}{t_p} = \frac{t}{t_s}$$   

$t$ - Serial time (**fixed**)  
$n$ - Number of chunks (or processes)  
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  

#### Strong Scaling: fixed work, increasing resources  

## Serial Section (Gustafson's Law) {background-image="/pics/parallel/Parallel2.png" background-size="100%" background-position="center" .smaller}  

$$\qquad$$  
$$\qquad$$
$$t_p = t_s + t_n$$  
$$\mbox{Speedup:}\quad\frac{t}{t_p} = \frac{t_s + nt_n}{t_s + t_n} = O(n)$$   

$t$ - Serial time (**growing**: $t_{2n} = 2t_n$ )  
$n$ - Number of chunks (or processes)  
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  

#### Weak Scaling: increasing work, increasing resources  

::: notes
Weak: misnomer: the speedup is actually great
:::

## Parallel Overhead {background-image="/pics/parallel/Parallel3.png" background-size="100%" background-position="center" .smaller}

$$\qquad$$  
$$\qquad$$  
$$t_p = t_s + t_n + t_o(n)$$  
$t$ - Serial time  
$n$ - Number of processes  
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  
$t_o(n)$ - Parallel overhead time  

## Replication of Serial Section (Distirbuted) {background-image="/pics/parallel/Parallel4.png" background-size="100%" background-position="center" .smaller}

$$\qquad$$  
$$\qquad$$  $$\qquad$$  $$\qquad$$  

$t$ - Serial time  
$n$ - Number of processes   
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  
$t_o(n)$ - Parallel overhead time  

#### Replication can reduce communication overhead due to communication

