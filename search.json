[
  {
    "objectID": "syllabus.html#background",
    "href": "syllabus.html#background",
    "title": "Syllabus",
    "section": "BACKGROUND",
    "text": "BACKGROUND\nThis course presents a high performance computing (HPC) view of some statistical and machine learning algorithms. It centers on a high-level view of today’s HPC architectures and parallel computing concepts for data science, along with practical exercises on a modern HPC cluster system.\n\nPrerequisites:\nThis course will build on the R content of BZAN 542. Students should have a basic understanding of R, at least at the level of Part I in Prof. Matloff’s fastR.\n\n\nHistory of this Course:\nIt is the first time this course has been offered so there will be some rough edges and some material provided “just in time.” Much of the material was recently presented in data science and HPC conference tutorials and is being adapted to this half-semester course. I welcome any suggestions and corrections to improve the material."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus",
    "section": "Software:",
    "text": "Software:\nOn your laptop, make sure you have a current version of (or install): R, RStudio, and git. Also, create a free account on GitHub if you don’t have one already.\nCode development will be presented with RStudio, so comfort with using RStudio helps, but other development environments (such as Jupyter) can be used. However, if you are not using RStudio, I may be of limited help in troubleshooting your IDE issues. Remote computing will be on the Delta cluster at the National Center for Supercomputing Applications (NCSA), which we will access with ssh and transfer codes via GitHub. You should have a GitHub personal account (free) and I will provide you with an account on Delta. Computing on the Delta cluster will be via submission of batch scripts to its Slurm workload manager."
  },
  {
    "objectID": "syllabus.html#grading-groups-assignments-and-project",
    "href": "syllabus.html#grading-groups-assignments-and-project",
    "title": "Syllabus",
    "section": "Grading, Groups, Assignments, and Project:",
    "text": "Grading, Groups, Assignments, and Project:\n\nGroups:\nThe class will be divided into groups of mostly 4, some 3, that are encouraged to work together. It is highly recommended for all group members to actively begin coding independently, discussing things but not just copying work of one group member. Your ability to work with HPC concepts after the class will depend directly on the amount of trial-and-error experience that you put into the assignments and project.\n\n\nAssignments:\nThis course will have 6 assignments (one each of the first 6 weeks) that are assigned on Monday and are due by 23:59 on the following Sunday. They may depend on material presented on Monday and Wednesday of the same week. If R code development is part of the assignment, it must reproduce your results when run by the instructor.\n\n\nProject:\nIn addition, there will be a group project culminating in a 5-minute lightning presentation in the last class on Monday, May 6.\nEach group’s project plan must be defined and approved by the end of the second week (Friday, March 29). Each group should make an appointment with the instructor to discuss their project proposal. The project will consist of a specific large data set of your choosing and a hypothesis to be examined with statistical and machine learning methods. The main objective is to successfully exercise several HPC methods (e.g. faster, parallel, analysis or larger data enabled by methods learned in class). A great discovery from the data is not required, although that would be the icing on the cake.\n\n\nGrading:\nThe weekly assignments are worth 12% each and the project with presentation is 25%. Assignments will vary individual submissions and group submissions. In group submissions, all members receive the same grade. At the end of the course all members will evaluate and specify the kind of their own and others’ relative contributions. The remaining 3% will be based on these evaluations and on class attendance, which will be taken at each lecture.\n\n\nLate submission policy:\n50% of the grade if submitted the following week. Zero after that."
  },
  {
    "objectID": "syllabus.html#data",
    "href": "syllabus.html#data",
    "title": "Syllabus",
    "section": "Data",
    "text": "Data\nSeveral cities have started publishing data about their operation and about various available services. Perhaps the best known is the New York City data, where TLC is a well-known very large taxi ride data set. Knoxville has also joined this trend Knoxville Open Data. Another interesting collection of data is at the University of Washington. A broader list of available data for projects is at careerfoundry and a somewhat overlapping Database Star. For your project, find a data set that is at least 1 GB in size."
  },
  {
    "objectID": "syllabus.html#optional-reading",
    "href": "syllabus.html#optional-reading",
    "title": "Syllabus",
    "section": "Optional Reading:",
    "text": "Optional Reading:\nHappy Git and GitHub for the useR\nThe Art of R Programming by Norman Matloff\nAdvanced R by Hadley Wickham.\nLarge Language Models and their use in [coding](https://michelnivard.github.io/gptstudio/\nR torch\nFor a deeper dive into HPC, consider The Science of Computing by Viktor Eijkhout with Edmond Chow and Robert van de Geijn."
  },
  {
    "objectID": "syllabus.html#interesting-links",
    "href": "syllabus.html#interesting-links",
    "title": "Syllabus",
    "section": "Interesting links:",
    "text": "Interesting links:\nThere is a number of great websites and podcasts that provide additional interesting information. See for instance R-bloggers, R Graph Gallery, Towards Data Science, R Weekly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HPC for Data Science in R - BZAN 583",
    "section": "",
    "text": "This course is intended to provide an ability to use modern HPC systems for data science with R, provide some understanding of computational and large data issues that arise in statistical and machine learning and their potential solutions, and give some practice of these skills on a modern HPC cluster system. See Syllabus link above for additional details.\nIt is the first time this course has been offered so there will be some rough edges and some material provided “just in time.” Much of the material was recently presented in data science and HPC conference tutorials and is being adapted to this half-semester course. I welcome any suggestions and corrections to improve the material.\nPlease check back frequently for updates! NOTE: This is a GitHub Pages site, which uses caching so you might need to click the refresh button within your web browser to see changes.\n\n\n\n\n\n\nWeek 1, March 18 & 20\n\n\nLectures: Welcome and Setup Workflow Introduction Setup and Demo\nAssignment: Installs and Workflow \n\n\n\n\nWeek 2, March 25 & 27\n\n\nLectures: Coding for Faster R, Memory Hierarchy and Hardware\nAssignment: Faster Code \n\n\n\n\nWeek 3, April 1 & 3\nProject Proposals Due by Friday, April 5.\n\n\nLectures: HPC Software, The Unix fork\nAssignment: Project Proposal \n\n\n\n\nWeek 4, April 8 & 10\n\n\nLectures: File System and Data, Parallel\nAssignment: Project Data\n\n\n\n\nWeek 5, April 15 & 17\n\n\nLectures: Methods that Rely on Sampling\nAssignment: Resampling Estimators\n\n\n\n\nWeek 6, April 22 & 24\n\n\nLectures: LLM or Torch for R BLAS Libraries\nAssignment?: Starter Code\n\n\n\n\nWeek 7, April 29 & May 1\n\n\nLectures: Distributed Computing\nAssignment?: pbdMPI in Project \n\n\n\n\nWeek 8, May 6\n\n\nProject: Presentations\n\n\n\nNotes: Programming Style"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#speeding-up-your-r-code",
    "href": "materials/lectures/2-faster/slides.html#speeding-up-your-r-code",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Speeding up your R code",
    "text": "Speeding up your R code\nSerial solutions before parallel solutions\n\nAny R code can be made faster\n\nHigh-level code == deep complexity\n\nProfile and improve code first\n\nVectorize loops if possible\n\nCompute once if not changing\n\nKnow when copies are made\n\nReduce generality to speed up\n\nAccess data contiguous order when possible\n\nMove kernels into compiled language, such as C/C++"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#performance-profiling-tools-system.time",
    "href": "materials/lectures/2-faster/slides.html#performance-profiling-tools-system.time",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Performance Profiling Tools system.time()\n",
    "text": "Performance Profiling Tools system.time()\n\n\nx = matrix(runif(20000*750), nrow=20000, ncol=750)\n\nsystem.time({xtx1 = t(x) %*% x})\n\n   user  system elapsed \n  0.094   0.005   0.098 \n\nsystem.time({xtx2 = crossprod(x)})\n\n   user  system elapsed \n  0.037   0.001   0.037 \n\nall.equal(xtx1, xtx2)\n\n[1] TRUE\n\nidentical(xtx1, xtx2)\n\n[1] FALSE\n\n\n\ncrossprod() knows \\(X^TX\\) is a symmetric matrix"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#performance-profiling-tools-rprof",
    "href": "materials/lectures/2-faster/slides.html#performance-profiling-tools-rprof",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Performance Profiling Tools: Rprof()\n",
    "text": "Performance Profiling Tools: Rprof()\n\nSamples the call stack at interval (default 0.02 second)\n\nx = matrix(rnorm(1e5*250), nrow=1e5, ncol=250)\n\nRprof(interval = 0.005)\nx_pr = prcomp(x, retx = FALSE)\nRprof(NULL)\nsummaryRprof()$by.self\n\n                self.time self.pct total.time total.pct\n\"La.svd\"            1.375    82.83      1.420     85.54\n\"aperm.default\"     0.130     7.83      0.130      7.83\n\"array\"             0.035     2.11      0.035      2.11\n\"is.finite\"         0.035     2.11      0.035      2.11\n\"colMeans\"          0.030     1.81      0.030      1.81\n\"any\"               0.020     1.20      0.020      1.20\n\"matrix\"            0.020     1.20      0.020      1.20\n\"sweep\"             0.010     0.60      0.175     10.54\n\"svd\"               0.005     0.30      1.455     87.65"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#performance-profiling-tools-rprof-1",
    "href": "materials/lectures/2-faster/slides.html#performance-profiling-tools-rprof-1",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Performance Profiling Tools: Rprof()\n",
    "text": "Performance Profiling Tools: Rprof()\n\n\nRprof(interval=.005)\nx_pr = prcomp(x, retx = FALSE)\nRprof(NULL)\nsummaryRprof()$by.self[1:5, ]     \n\n                self.time self.pct total.time total.pct\n\"La.svd\"            1.390    85.28      1.425     87.42\n\"aperm.default\"     0.140     8.59      0.140      8.59\n\"array\"             0.030     1.84      0.030      1.84\n\"is.finite\"         0.030     1.84      0.030      1.84\n\"any\"               0.015     0.92      0.015      0.92\n\n\n\n\n                self.time self.pct total.time total.pct\n\"La.svd\"            1.395    84.29      1.440     87.01\n\"aperm.default\"     0.140     8.46      0.140      8.46\n\"array\"             0.025     1.51      0.025      1.51\n\"is.finite\"         0.025     1.51      0.025      1.51\n\"matrix\"            0.025     1.51      0.025      1.51\n\n\n\n\n                self.time self.pct total.time total.pct\n\"La.svd\"            1.400    84.85      1.450     87.88\n\"aperm.default\"     0.135     8.18      0.135      8.18\n\"is.finite\"         0.030     1.82      0.030      1.82\n\"array\"             0.025     1.52      0.025      1.52\n\"matrix\"            0.025     1.52      0.025      1.52"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#lazy-evaluation",
    "href": "materials/lectures/2-faster/slides.html#lazy-evaluation",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\nFunction parameters are not evaluated until needed!\n\n\nsquare = function(x) { # \"pass-by-value\" but no x copy made\n  x = x*x              # x is just used, not copied \n  x\n}\n\n\nCommon in functional languages\nSpeeds up computation by\n\nBetter memory access patterns\nPrevents extra copies of data\nNot wasting computation on things never used\n\n\n\nTime or profile more than once to remove possible lazy evaluation effects\nRead more: About lazy evaluation (R-bloggers), How environments work in R and what is lazy evaluation (R-bloggers)"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#performance-profiling-tools-rbenchmark",
    "href": "materials/lectures/2-faster/slides.html#performance-profiling-tools-rbenchmark",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Performance Profiling Tools: rbenchmark\n",
    "text": "Performance Profiling Tools: rbenchmark\n\na package that easily benchmarks different functions\n\nx = matrix(rnorm(1000*500), nrow=1000, ncol=500)\n\nf = function(x) t(x) %*% x\ng = function(x) crossprod(x)\n\nlibrary(rbenchmark)\nbenchmark(f(x), g(x))\n\n  test replications elapsed relative user.self sys.self user.child sys.child\n1 f(x)          100   0.226    2.511     0.200    0.027          0         0\n2 g(x)          100   0.090    1.000     0.083    0.006          0         0"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#profiling-summary",
    "href": "materials/lectures/2-faster/slides.html#profiling-summary",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Profiling Summary",
    "text": "Profiling Summary\n\nProfile, profile, profile\n\nUse system.time() to get a general sense of a method\n\nUse rbenchmark’s benchmark() function to compare 2 methods\nUse Rprof() for more detailed profiling\nOther tools exist for more advanced and graphical output: profvis()"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#vectorizing",
    "href": "materials/lectures/2-faster/slides.html#vectorizing",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Vectorizing",
    "text": "Vectorizing\n\nn = 1e6\nx = seq(0, 1, length.out=n)\nf = function(x) exp(x^3 + 2.5*x^2 + 12*x + 0.12)\ny1 = numeric(n)\n\nset.seed(12345)\nsystem.time({\n  for(i in seq_len(n))\n    y1[i] = f(x[i]) + rnorm(1)\n})\n\n   user  system elapsed \n  0.950   0.070   1.021 \n\nset.seed(12345)\nsystem.time({\n  y2 = f(x) + rnorm(n)\n})\n\n   user  system elapsed \n  0.030   0.001   0.031 \n\nall.equal(y1, y2)\n\n[1] TRUE"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#compute-once-if-not-changing",
    "href": "materials/lectures/2-faster/slides.html#compute-once-if-not-changing",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Compute Once if not Changing",
    "text": "Compute Once if not Changing\nA is a very large matrix\nfor (i in seq_len(n)){\n  Y = t(A) %*% Q\n  Q = qr.Q(qr(Y))\n  Y = A %*% Q\n  Q = qr.Q(qr(Y))\n}\nMove the transpose outside the loop:\ntA = t(A)\nfor (i in seq_len(n)){\n  Y = tA %*% Q\n  Q = qr.Q(qr(Y))\n  Y = A %*% Q\n  Q = qr.Q(qr(Y))\n}"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#check-that-faster-code-is-still-correct",
    "href": "materials/lectures/2-faster/slides.html#check-that-faster-code-is-still-correct",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Check that Faster Code is Still Correct",
    "text": "Check that Faster Code is Still Correct\nfor (i in seq_len(n)){\n  Y = t(A) %*% Q\n  Q = qr.Q(qr(Y))\n  Y = A %*% Q\n  Q = qr.Q(qr(Y))\n}\nQ1 = Q\n\ntA = t(A)\nfor (i in seq_len(n)){\n  Y = tA %*% Q\n  Q = qr.Q(qr(Y))\n  Y = A %*% Q\n  Q = qr.Q(qr(Y))\n}\nQ2 = Q\nall.equal(Q1, Q2)\nidentical(Q1, Q2)"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#example-from-a-real-r-package-on-cran",
    "href": "materials/lectures/2-faster/slides.html#example-from-a-real-r-package-on-cran",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Example from a Real R Package on CRAN",
    "text": "Example from a Real R Package on CRAN\n# ...\nwhile(i&lt;=N){\n  for(j in 1:i){\n    d.k = as.matrix(x)[l==j,l==j]}\n# ...\nConvert to matrix outside the loop:\n# ...\nx.mat = as.matrix(x)\nwhile(i&lt;=N){\n  for(j in 1:i){\n    d.k = x.mat[l==j,l==j]\n# ...\nBy changing just 1 line of code, performance of the main method of the package improved over 3.5x !"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#preallocate-lists-and-vectors-instead-of-growing-them",
    "href": "materials/lectures/2-faster/slides.html#preallocate-lists-and-vectors-instead-of-growing-them",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Preallocate lists and vectors Instead of Growing Them",
    "text": "Preallocate lists and vectors Instead of Growing Them\n\nm = 1e3\nn = 1e5\nmat = matrix(runif(m*n), nrow = m)\nsystem.time({\nresult1 = NULL\nfor(i in seq_len(n)) {\n    newres = mean(mat[, i])\n    result1 = c(result1, newres)\n}})\n\n   user  system elapsed \n  6.431   1.832   8.277 \n\nsystem.time({\nresult2 = vector(\"double\", n)\nfor(i in seq_len(n)) {\n    result2[i] = mean(mat[, i])\n}})\n\n   user  system elapsed \n  0.406   0.048   0.453 \n\nall.equal(result1, result2)\n\n[1] TRUE"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#specialize-more-general-functions",
    "href": "materials/lectures/2-faster/slides.html#specialize-more-general-functions",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Specialize More General Functions",
    "text": "Specialize More General Functions\n\nx = matrix(runif(20000*750), nrow=20000, ncol=750)\n\nsystem.time({cx1 = cov(x)})\n\n   user  system elapsed \n  3.857   0.001   3.861 \n\nsystem.time({cx2 = crossprod(sweep(x, 2, colMeans(x)))/(nrow(x) - 1)})\n\n   user  system elapsed \n  0.109   0.007   0.115 \n\nall.equal(cx1, cx2)\n\n[1] TRUE\n\n\n\\(Cov(X) = \\frac{(X - {\\bf 1}\\bar{x})^T(X - {\\bf 1}\\bar{x})}{n - 1},\\) where \\(\\bar{x}\\) is a row vector of column means of \\(X\\) and \\(\\bf 1\\) is a column of 1s."
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#memory-hierarchy-considerations-with-matrices-and-data.frames",
    "href": "materials/lectures/2-faster/slides.html#memory-hierarchy-considerations-with-matrices-and-data.frames",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Memory Hierarchy Considerations with Matrices and data.frames",
    "text": "Memory Hierarchy Considerations with Matrices and data.frames\n\nR matrices are stored by columns\nR data.frames (lists) are stored by variables\n\n\nmemuse::memuse(mat)\n\n762.940 MiB\n\n\n\nColumns contain contiguous data\n\nRows are scattered across columns\n\nR, Julia, Matlab, and Fortran are column-major\nPython and C are row-major"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#contiguous-access-to-arrays-and-lists",
    "href": "materials/lectures/2-faster/slides.html#contiguous-access-to-arrays-and-lists",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Contiguous access to arrays and lists",
    "text": "Contiguous access to arrays and lists\n\nsystem.time({\nresult1 = vector(\"double\", m)\nfor(i in seq_len(m)) {\n    result1[i] = mean(mat[i, ])  # non-contiguous access by rows\n}})\n\n   user  system elapsed \n  0.857   0.067   0.926 \n\nsystem.time({tmat = t(mat)}) # transpose for columnar access\n\n   user  system elapsed \n  0.369   0.043   0.411 \n\nsystem.time({\nresult2 = vector(\"double\", m)\nfor(i in seq_len(m)) {\n    result2[i] = mean(tmat[, i]) # contiguous access via columns\n}})\n\n   user  system elapsed \n  0.231   0.058   0.289 \n\nall.equal(result1, result2)\n\n[1] TRUE"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#programming-languages-hierarchy",
    "href": "materials/lectures/2-faster/slides.html#programming-languages-hierarchy",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "Programming Languages Hierarchy",
    "text": "Programming Languages Hierarchy\n\n\nHigh-level, Scripting Languages: R, Python, …\n\nEfficient for vectors, matrices, and arrays\nInterpreted at runtime to run pre-compiled executables\n\n\n\nMid-level, Compiled Languages: C, C++, Fortran, …\n\nFast enough for loops through single elements of long vectors\nNeed compiling and linking to machine libraries before they run\nStill machine and operating system independent\n\n\n\nLow-level, Machine code\n\nHardware and operating system specific\nManipulating data between registers and memory hierarchies"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#rcpp-incorporating-c-code-into-r",
    "href": "materials/lectures/2-faster/slides.html#rcpp-incorporating-c-code-into-r",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "\nRcpp: Incorporating C++ Code into R",
    "text": "Rcpp: Incorporating C++ Code into R\n\nSimplifies integrating C++ code with R\nMaps R objects (vectors, matrices, functions, environments, etc.) to C++ classes\nCode is compiled, linked, and loaded on the fly, or added via packages\nAppropriate when custom element-wise operations cannot be vectorized with R\n\nRead Advanced R: High performance functions with Rcpp by Hadley Wickham"
  },
  {
    "objectID": "materials/lectures/2-faster/slides.html#rfast-and-rfast2-packages-a-collection-of-rcpp-implemented-functions",
    "href": "materials/lectures/2-faster/slides.html#rfast-and-rfast2-packages-a-collection-of-rcpp-implemented-functions",
    "title": "Faster R for All Platforms, Including HPC Clusters",
    "section": "\nRfast and RFast2 Packages: A collection of Rcpp-Implemented Functions",
    "text": "Rfast and RFast2 Packages: A collection of Rcpp-Implemented Functions\n\nsystem.time({cx2 = crossprod(sweep(x, 2, colMeans(x)))/(nrow(x) - 1)})\n\n   user  system elapsed \n  0.326   0.017   0.350 \n\nsystem.time({cx3 = Rfast::cova(x)})\n\n   user  system elapsed \n  0.064   0.004   0.070 \n\nall.equal(cx2, cx3)\n\n[1] TRUE\n\n\nTaking R to its limits: 70+ tips by Michail Tsagris and Manos Papadakis is a “not peer-reviewed” PeerJPreprints publication.\nLots of good advice Some good and some bad programming practice Some spelling and grammar issues Definitely worth reading!"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#today",
    "href": "materials/lectures/1b-setupdemo/slides.html#today",
    "title": "Setup Continued and Demo",
    "section": "Today",
    "text": "Today\n\nDelta logins\n\nOffice hours\n\nSummary of survey\n\nHPCinR organization and Quarto course materials\n\nCode repository copy and workflow demo"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#delta-logins",
    "href": "materials/lectures/1b-setupdemo/slides.html#delta-logins",
    "title": "Setup Continued and Demo",
    "section": "Delta logins",
    "text": "Delta logins\nACCESS is a project, funded by the National Science Foundation, that helps researchers and educators utilize NSF-funded HPC centers.\n\nThe forgot the students\n\n\nNCSA is one of these HPC centers at University of Illinois\n\nDelta is NCSA’s (and NSF’s) largest cluster\nDelta allocates mostly via ACCESS but also locally\nBoth ACCESS and NCSA/Delta have an “add user” function"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#office-hours",
    "href": "materials/lectures/1b-setupdemo/slides.html#office-hours",
    "title": "Setup Continued and Demo",
    "section": "Office hours",
    "text": "Office hours\nYour class schedule\n\nM\nT\nW\nR"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#survey",
    "href": "materials/lectures/1b-setupdemo/slides.html#survey",
    "title": "Setup Continued and Demo",
    "section": "Survey",
    "text": "Survey\n\nSummary\n\nDo you use a text editor?\n\nWhich one? vim, emacs, nano, …\n\nRstudio is a text editor with extras"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#workflow-review",
    "href": "materials/lectures/1b-setupdemo/slides.html#workflow-review",
    "title": "Setup Continued and Demo",
    "section": "Workflow review",
    "text": "Workflow review\n\nThree git repositories\n\nOn laptop - GitHub is set as remote\nOn GitHub\nOn login.delta.ncsa.illinois.edu - GitHub is set as remote"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#quarto-course-materials",
    "href": "materials/lectures/1b-setupdemo/slides.html#quarto-course-materials",
    "title": "Setup Continued and Demo",
    "section": "Quarto Course Materials",
    "text": "Quarto Course Materials\n\nCourse materials are on GitHub/HPCinR/BZAN_583\n\nQuarto website with embedded presentations\nA template repository\n\n\n\nUsed .Rmd and xaringan combination before Quarto combines them and embraces other lanuages"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#code-repository-template",
    "href": "materials/lectures/1b-setupdemo/slides.html#code-repository-template",
    "title": "Setup Continued and Demo",
    "section": "Code Repository Template",
    "text": "Code Repository Template\nHPCinR/BZAN_583_code\n- On GitHub: Copy template to your account\n- Clone to your laptop\n- Make edits\n- Add, commit, and push edits to GitHub\n- Get clone link from GitHub - Login to Delta\n- Clone to your Delta files\n- Run script (is it executable? chmod +x)"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/slides.html#ready-for-workflow-demo",
    "href": "materials/lectures/1b-setupdemo/slides.html#ready-for-workflow-demo",
    "title": "Setup Continued and Demo",
    "section": "Ready for Workflow Demo",
    "text": "Ready for Workflow Demo\nNeed:\n- Laptop RStudio open on repo project\n- ssh terminal in repo directory on Delta\nWorkflow:\n- Edit code in RStudio on laptop\n- Add, commit, and push edits to GitHub\n- Pull code on Delta\n- Run code and repeat workflow"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#file-systems-on-delta",
    "href": "materials/lectures/4-data/slides.html#file-systems-on-delta",
    "title": "Data",
    "section": "File Systems on Delta",
    "text": "File Systems on Delta\n\nOur project name on Delta is bckj.\nEach user has access to three file systems /u/&lt;username&gt;, /projects/bckj/&lt;username&gt;, and /scratch/bcjk/&lt;username&gt;\nSee Delta: File Systems, andDelta: Data Management for more details.\nNone of the Delta file systems are backed up - another good reason to work via GitHub."
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#permissions-on-the-directories",
    "href": "materials/lectures/4-data/slides.html#permissions-on-the-directories",
    "title": "Data",
    "section": "Permissions on the directories",
    "text": "Permissions on the directories\n(gostrouc user view):\n\nls -ld $HOME\n\n\ndrwxrwx---+ 9 root root 4096 Mar 29 16:14 /u/gostrouc/\n\n\nls -ld /projects/bckj\n\n\ndrwxrws---+ 26 root delta_bckj 12288 Mar 26 04:55 /projects/bckj/\n\n\nls -ld /scratch/bckj\n\n\ndrwxrws---+ 26 root delta_bckj 8192 Mar 26 04:55 /scratch/bckj/\n\nd|uuu|ggg|ooo \\(\\quad\\)\ndir|user|group|others\nExec permissions (from man page):\n- x The execute/search bits\n- s The set-user-ID-on-execution and set-group-ID-on-execution bits"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#projectsbcjk",
    "href": "materials/lectures/4-data/slides.html#projectsbcjk",
    "title": "Data",
    "section": "/projects/bcjk",
    "text": "/projects/bcjk\n\nls -l /projects/bckj\n\n\ntotal 96\ndrwxrws---+ 2 aholme27         delta_bckj 4096 Mar 19 14:55 aholme27/\ndrwxrws---+ 2 asajjad          delta_bckj 4096 Mar 21 11:55 asajjad/\ndrwxrws---+ 2 ballred          delta_bckj 4096 Mar 21 10:55 ballred/\ndrwxrws---+ 2 briddle          delta_bckj 4096 Mar 26 03:55 briddle/\ndrwxrws---+ 2 carolinetabeling delta_bckj 4096 Mar 21 10:56 carolinetabeling/\ndrwxrws---+ 2 cdeaton3         delta_bckj 4096 Mar 21 10:55 cdeaton3/\ndrwxrws---+ 2 cking134         delta_bckj 4096 Mar 19 14:55 cking134/\ndrwxrws---+ 2 gostrouc         delta_bckj 4096 Feb  3 03:55 gostrouc/\ndrwxrws---+ 2 hjohns61         delta_bckj 4096 Mar 25 16:55 hjohns61/\ndrwxrws---+ 2 ianleonard       delta_bckj 4096 Mar 21 10:56 ianleonard/\ndrwxrws---+ 2 jcamilleri       delta_bckj 4096 Mar 21 10:56 jcamilleri/\ndrwxrws---+ 2 jgary4           delta_bckj 4096 Mar 21 10:56 jgary4/\ndrwxrws---+ 2 jmillic4         delta_bckj 4096 Mar 19 14:55 jmillic4/\ndrwxrws---+ 2 jpeta            delta_bckj 4096 Mar 21 10:55 jpeta/\ndrwxrws---+ 2 kpatel8          delta_bckj 4096 Mar 21 12:55 kpatel8/\ndrwxrws---+ 2 lhu7             delta_bckj 4096 Mar 21 10:55 lhu7/\ndrwxrws---+ 2 lmcpherr         delta_bckj 4096 Mar 21 10:56 lmcpherr/\ndrwxrws---+ 2 lschwar4         delta_bckj 4096 Mar 19 14:55 lschwar4/\ndrwxrws---+ 2 mrobiso4         delta_bckj 4096 Mar 26 04:55 mrobiso4/\ndrwxrws---+ 2 sp28             delta_bckj 4096 Mar 26 03:55 sp28/\ndrwxrws---+ 2 sthompson2       delta_bckj 4096 Mar 26 04:55 sthompson2/\ndrwxrws---+ 2 tdykes2          delta_bckj 4096 Mar 21 10:55 tdykes2/\ndrwxrws---+ 2 wjeter           delta_bckj 4096 Mar 21 10:56 wjeter/\ndrwxrws---+ 2 ychang27         delta_bckj 4096 Mar 19 14:55 ychang27/"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#scratchbcjk",
    "href": "materials/lectures/4-data/slides.html#scratchbcjk",
    "title": "Data",
    "section": "/scratch/bcjk",
    "text": "/scratch/bcjk\n\nls -l /scratch/bckj\n\n\ntotal 96\ndrwxrws---+ 2 aholme27         delta_bckj 4096 Mar 19 14:55 aholme27/\ndrwxrws---+ 2 asajjad          delta_bckj 4096 Mar 21 11:55 asajjad/\ndrwxrws---+ 2 ballred          delta_bckj 4096 Mar 21 10:55 ballred/\ndrwxrws---+ 2 briddle          delta_bckj 4096 Mar 26 03:55 briddle/\ndrwxrws---+ 2 carolinetabeling delta_bckj 4096 Mar 21 10:56 carolinetabeling/\ndrwxrws---+ 2 cdeaton3         delta_bckj 4096 Mar 21 10:55 cdeaton3/\ndrwxrws---+ 2 cking134         delta_bckj 4096 Mar 19 14:55 cking134/\ndrwxrws---+ 2 gostrouc         delta_bckj 4096 Feb  3 03:55 gostrouc/\ndrwxrws---+ 2 hjohns61         delta_bckj 4096 Mar 25 16:55 hjohns61/\ndrwxrws---+ 2 ianleonard       delta_bckj 4096 Mar 21 10:56 ianleonard/\ndrwxrws---+ 2 jcamilleri       delta_bckj 4096 Mar 21 10:56 jcamilleri/\ndrwxrws---+ 2 jgary4           delta_bckj 4096 Mar 21 10:56 jgary4/\ndrwxrws---+ 2 jmillic4         delta_bckj 4096 Mar 19 14:55 jmillic4/\ndrwxrws---+ 2 jpeta            delta_bckj 4096 Mar 21 10:55 jpeta/\ndrwxrws---+ 2 kpatel8          delta_bckj 4096 Mar 21 12:55 kpatel8/\ndrwxrws---+ 2 lhu7             delta_bckj 4096 Mar 21 10:55 lhu7/\ndrwxrws---+ 2 lmcpherr         delta_bckj 4096 Mar 21 10:56 lmcpherr/\ndrwxrws---+ 2 lschwar4         delta_bckj 4096 Mar 19 14:55 lschwar4/\ndrwxrws---+ 2 mrobiso4         delta_bckj 4096 Mar 26 04:55 mrobiso4/\ndrwxrws---+ 2 sp28             delta_bckj 4096 Mar 26 03:55 sp28/\ndrwxrws---+ 2 sthompson2       delta_bckj 4096 Mar 26 04:55 sthompson2/\ndrwxrws---+ 2 tdykes2          delta_bckj 4096 Mar 21 10:55 tdykes2/\ndrwxrws---+ 2 wjeter           delta_bckj 4096 Mar 21 10:56 wjeter/\ndrwxrws---+ 2 ychang27         delta_bckj 4096 Mar 19 14:55 ychang27/"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#using-the-file-systems",
    "href": "materials/lectures/4-data/slides.html#using-the-file-systems",
    "title": "Data",
    "section": "Using the File Systems",
    "text": "Using the File Systems\n\nCode is typically maintained in /u/&lt;username&gt;\n\nProject data is usually kept in /projects/bckj/ subdirectories\n\nThis is also a place for project team accessible code\n\n\nRuntime temporary data can be written to /scratch/bckj/\n\n\n\n[gostrouc@dt-login04 data]$ ls -ld /projects/bckj/Team*\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  8 09:32 /projects/bckj/Team1/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  8 09:32 /projects/bckj/Team2/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  8 09:32 /projects/bckj/Team3/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  8 09:32 /projects/bckj/Team4/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  8 09:32 /projects/bckj/Team5/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  8 09:32 /projects/bckj/Team6/"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#bringing-output-back-to-laptop",
    "href": "materials/lectures/4-data/slides.html#bringing-output-back-to-laptop",
    "title": "Data",
    "section": "Bringing Output Back to Laptop",
    "text": "Bringing Output Back to Laptop\nNCSA recommendations\n\nWindows users: WinSCP or Cuberduck\n\nMac users: in laptop Terminal\n\n\nscp &lt;user&gt;@login.delta.ncsa.illinois.edu:&lt;file-path&gt; &lt;dest-path&gt;\n\n\n&lt;file-path&gt; is relative to home directory on Delta or an absolute path (starts with /) on Delta\n\n\n&lt;dest-path&gt; is on your laptop"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#reading-data-overview",
    "href": "materials/lectures/4-data/slides.html#reading-data-overview",
    "title": "Data",
    "section": "Reading Data Overview",
    "text": "Reading Data Overview\n\nText files (.csv, .tsv, .txt, … ): variable length records\n\n\ndata.table::fread()  # fastest reader\nreadr::read_delim()  # fast reader    \n\nBinary file formats faster for large data (& smaller): fixed length records\n\n\narrow::read_parquet()  # chunked column-oriented format (.parquet extension)\n\nArrow for R Cheatsheet\n\nrhdf5::h5read()  # hierarchical array-oriented format (.h5 extension)\n\nrhdf5 Practical Tips on Bioconductor\nOther formats (.xlsx, etc. )"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#downloading-data-overview",
    "href": "materials/lectures/4-data/slides.html#downloading-data-overview",
    "title": "Data",
    "section": "Downloading Data Overview",
    "text": "Downloading Data Overview\nUnix: wget, curl, scp\nR: utils::download.file()"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#new-york-tlc-trip-record-data",
    "href": "materials/lectures/4-data/slides.html#new-york-tlc-trip-record-data",
    "title": "Data",
    "section": "New York TLC Trip Record Data",
    "text": "New York TLC Trip Record Data\n\nTLC: Taxi and Limousine Commission\nIncludes: pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts\nData overview\nYellow Taxi Trip Data Dictionary\nGreen Taxi Trips Data Dictionary\nFHV Trips Data Dictionary\nHigh Volume FHV Trips Data Dictionary"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data",
    "href": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data",
    "title": "Data",
    "section": "Yellow Taxi Trip Data",
    "text": "Yellow Taxi Trip Data\nYellow taxi trip data is downloaded on Delta in parquet format with an arrow-friendly directory structure.\n\n[gostrouc@dt-login03 ~]$ cd /projects/bckj/TLC_yellow\n[gostrouc@dt-login03 TLC_yellow]$ ls -l\ntotal 60\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:06 'year=2009'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:07 'year=2010'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:08 'year=2011'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:09 'year=2012'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:09 'year=2013'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:09 'year=2014'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:09 'year=2015'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2016'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2017'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2018'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2019'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2020'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2021'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 21:10 'year=2022'/\ndrwxrws---+ 14 gostrouc delta_bckj 4096 Apr  7 22:43 'year=2023'/"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data-projectsbckjtlc_yellow",
    "href": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data-projectsbckjtlc_yellow",
    "title": "Data",
    "section": "Yellow Taxi Trip Data /projects/bckj/TLC_yellow\n",
    "text": "Yellow Taxi Trip Data /projects/bckj/TLC_yellow\n\n\n[gostrouc@dt-login03 TLC_yellow]$ du -hs ./*\n5.4G    ./year=2009\n5.3G    ./year=2010\n2.1G    ./year=2011\n2.1G    ./year=2012\n2.0G    ./year=2013\n2.1G    ./year=2014\n1.9G    ./year=2015\n1.8G    ./year=2016\n1.5G    ./year=2017\n1.4G    ./year=2018\n1.2G    ./year=2019\n358M    ./year=2020\n458M    ./year=2021\n587M    ./year=2022\n[gostrouc@dt-login03 TLC_yellow]$ du -hs .\n29G ."
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data-2009",
    "href": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data-2009",
    "title": "Data",
    "section": "Yellow Taxi Trip Data 2009\n",
    "text": "Yellow Taxi Trip Data 2009\n\n\n[gostrouc@dt-login03 TLC_yellow]$ ls -l 'year=2009'\ntotal 48\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:05 'month=1'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:06 'month=10'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:06 'month=11'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:06 'month=12'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:05 'month=2'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:05 'month=3'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:05 'month=4'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:05 'month=5'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:05 'month=6'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:06 'month=7'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:06 'month=8'/\ndrwxrws---+ 2 gostrouc delta_bckj 4096 Apr  7 21:06 'month=9'/"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data-2009-01",
    "href": "materials/lectures/4-data/slides.html#yellow-taxi-trip-data-2009-01",
    "title": "Data",
    "section": "Yellow Taxi Trip Data 2009-01\n",
    "text": "Yellow Taxi Trip Data 2009-01\n\n\n[gostrouc@dt-login03 TLC_yellow]$ ls -l 'year=2009/month=1'\ntotal 458756\n-rw-rw----+ 1 gostrouc delta_bckj 469759995 Apr  7 21:05 yellow_tripdata_2009-01.parquet\n[gostrouc@dt-login03 TLC_yellow]$ ls -l 'year=2009/month=2'\ntotal 433048\n-rw-rw----+ 1 gostrouc delta_bckj 443434087 Apr  7 21:05 yellow_tripdata_2009-02.parquet\n[gostrouc@dt-login03 TLC_yellow]$ ls -l 'year=2009/month=3'\ntotal 471288\n-rw-rw----+ 1 gostrouc delta_bckj 482592227 Apr  7 21:05 yellow_tripdata_2009-03.parquet\n[gostrouc@dt-login03 TLC_yellow]$ ls -lh 'year=2009/month=11'\ntotal 457M\n-rw-rw----+ 1 gostrouc delta_bckj 457M Apr  7 21:06 yellow_tripdata_2009-11.parquet\n[gostrouc@dt-login03 TLC_yellow]$ ls -lh 'year=2009/month=12'\ntotal 466M\n-rw-rw----+ 1 gostrouc delta_bckj 465M Apr  7 21:06 yellow_tripdata_2009-12.parquet"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#curl-yellow-taxi-trip-data",
    "href": "materials/lectures/4-data/slides.html#curl-yellow-taxi-trip-data",
    "title": "Data",
    "section": "\ncurl Yellow Taxi Trip Data",
    "text": "curl Yellow Taxi Trip Data\n\n#' tlc_get_ym: Gets one month of TLC yellow taxi data and writes it in Hive\n#' style directory names, such as `dest/year=2015/month=8/file_name.parquet`.\n#' \n#' @param ymd\n#' A **lubridate** \"YYYY-MM-01\" giving the year and month to be retrieved.\n#' @param name\n#' Character file name base (such as 'yellow_tripdata') from the TLC \n#' repository documentation.\n#' @param dest\n#' Character destination directory path.\n#' @param url\n#' Character URL of data source location.\n#' \n#' @details\n#' Uses `method = curl` to get one month of TLC yellow taxi data into the\n#' `dest` directory. Since `download.file()` engages `curl`\n#' with a `system()` call, generating these calls to `tlcYM_get()` can be \n#' completely parallel and the data never touches R. \n#' \n#' @returns \n#' Invisibly, returns the character URL/file combination used in data retrieval.\n#' \ntlc_get_ym = function(ymd, name = \"yellow_tripdata\", \n                     dest = \"/projects/bckj/TLC_yellow\",\n                     url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\") {\n  yr = lubridate::year(ymd)\n  month = lubridate::month(ymd)\n\n  ## Construct source url/file\n  file_name = paste0(name, \"_\", yr, '-', sprintf('%02d', month), '.parquet')\n  file_url = paste0(url, '/', file_name)\n  \n  ## Construct destination directory\n  dest_dir = paste0(dest, \"/year=\", yr)\n  if(!dir.exists(dest_dir)) dir.create(dest_dir)\n  dest_dir = paste0(dest_dir, \"/month=\", month)\n  if(!dir.exists(dest_dir)) dir.create(dest_dir)\n  \n  download.file(file_url, paste0(dest_dir, \"/\", file_name), method = \"curl\",\n                quiet = TRUE)\n  invisible(file_url)\n}\n\n#' tlc_get_range: A wrapper for `tlc_get_ym()` to get a range of months and do\n#' it in parallel.\n#' \n#' @param first\n#' Character \"YYYY-MM\" first month. Uses `lubridate::ym()` to read\n#' @param last\n#' Character \"YYYY-MM\" last month\n#' @param cores\n#' Integer number of cores to use for running `curl` instances in parallel\n#' \n#' @details\n#' Since the function does a system call to `curl`, its time does not include\n#' the core time used by `curl` and includes only the R function time. But\n#' the *real* or *elapsed* time is correct.\n#' \n#' @returns\n#' Invisibly returns the full vector of months retrieved\n#' \ntlc_get_range = function(first, last, cores = 1) {\n  ## construct vector of yr-month-day requests\n  dates = seq(lubridate::ym(first), lubridate::ym(last), \"months\")\n  parallel::mclapply(dates, tlc_get_ym, mc.cores = cores)\n  invisible(dates)\n}\n\n# tlc_get_range(\"2021-01\", \"2022-12\", cores = 1)"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#compare-csv-to-parquet-reads-in-r",
    "href": "materials/lectures/4-data/slides.html#compare-csv-to-parquet-reads-in-r",
    "title": "Data",
    "section": "Compare csv to parquet reads in R",
    "text": "Compare csv to parquet reads in R\narrow (parquet 0.423GB, csv 1.7GB)\n\n&gt; system.time({tlc2009.02arr = arrow::read_parquet(\"/projects/bckj/TLC_yellow/year=2009/month=2/yellow_tripdata_2009-02.parquet\")})\n   user  system elapsed \n  3.363   1.981  12.236 \n&gt; system.time({tlc2009.02arrcsv = arrow::read_csv_arrow(\"/projects/bckj/TLC_yellow_csv/year=2009/month=2/part-0.csv\")})\n   user  system elapsed \n 20.513  32.494  33.163 \n\nreadr (csv 1.7GB)\n\n&gt; system.time({tlc2009.02csv = readr::read_csv(\"/projects/bckj/TLC_yellow_csv/year=2009/month=2/part-0.csv\")})\nRows: 14092413 Columns: 18                                                                                         \n── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (2): vendor_name, Payment_Type\ndbl  (11): Passenger_Count, Trip_Distance, Start_Lon, Start_Lat, End_Lon, En...\nlgl   (3): Rate_Code, store_and_forward, mta_tax\ndttm  (2): Trip_Pickup_DateTime, Trip_Dropoff_DateTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n   user  system elapsed \n 56.194  24.529  51.976 \n\ndata.table (csv 1.7GB)\n\n&gt; system.time({tlc2009.02dtcsv = data.table::fread(\"/projects/bckj/TLC_yellow_csv/year=2009/month=2/part-0.csv\")})\n|--------------------------------------------------|\n|==================================================|\n   user  system elapsed \n 29.299  28.551  25.260"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#convert-your-csv-data-to-parquet",
    "href": "materials/lectures/4-data/slides.html#convert-your-csv-data-to-parquet",
    "title": "Data",
    "section": "Convert your csv data to parquet\n",
    "text": "Convert your csv data to parquet\n\n\nlibrary(arrow)\nlibrary(dplyr)\n\ntlc = open_csv_dataset(\"/projects/bckj/TLC_yellow_csv/year=2009\")\n\nwrite_parquet(tlc, sink = \"/scratch/bckj/TLCtest\")\n\n\nShould work but currently fails on TLC_yellow_csv. Possibly due to second/partial conversion parquet to csv to part-parquet?\nConsequently, this becomes an optional portion of Assignment 4."
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#memory-use-by-r",
    "href": "materials/lectures/4-data/slides.html#memory-use-by-r",
    "title": "Data",
    "section": "Memory Use by R",
    "text": "Memory Use by R\nmemuse package\n\n\nmemuse::mu(): memory used by an object\n\nAdding option prefix = \"SI\" uses SI units (1e3 multiples, not 2^10 multiples)\n\n\n\nmemuse::Sys.procmem(): memory of the current R process\n\n\nmemuse::Sys.meminfo(): total available memory, and free memory\n\npryr package\n\n\npryr::object_size(): memory used by an object\n\n\npryr::mem_used(): memory of the current R process\n\nThe two packages can disagree with each other and with Unix. Try\n- system2(\"ps\", paste(\"-o drs,rss -p\", Sys.getpid()))\n\n\npryr: “The details of R’s memory management are not documented in a single place. Most of the information in this chapter was gleaned from a close reading of the documentation (particularly ?Memory and ?gc), the memory profiling section of R-exts, and the SEXPs section of R-ints. The rest I figured out by reading the C source code, performing small experiments, and asking questions on R-devel. Any mistakes are entirely mine.” Hadley Wickham, Advanced R (Same comment could be made by the memuse package author, but I find it agrees with Unix more often)"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#a-note-about-words",
    "href": "materials/lectures/4-data/slides.html#a-note-about-words",
    "title": "Data",
    "section": "A Note about Words",
    "text": "A Note about Words\n“batch processing”\n- HPC: running scripts non-interactively (whole) - arrow package: reading data in chunks (by batches, not whole)\n“fork”\n- HPC: Unix fork parallel process spawning\n- GitHub: Creating a repository fork ( making a copy for concurrent development)"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#reading-data",
    "href": "materials/lectures/4-data/slides.html#reading-data",
    "title": "Data",
    "section": "Reading Data",
    "text": "Reading Data\nSee examples in BZAN_583_code/data\n\n\nexploreTLC.R and exploreTLC.sh\n\n\nfread_exploreTLC.R and fread_exploreTLC.sh\n\n\nMPIexploreTLC.R and MPIexploreTLC.sh (incomplete)"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#exploretlc.r-2-months",
    "href": "materials/lectures/4-data/slides.html#exploretlc.r-2-months",
    "title": "Data",
    "section": "\nexploreTLC.R: 2 months",
    "text": "exploreTLC.R: 2 months\n\n[gostrouc@dt-login03 data]$ cat utk.o\n/u/gostrouc/BZAN_583_code/data\nloaded R\nSize:  146.953 MiB \nPeak:  146.953 MiB \nTotalram:  251.600 GiB \nFreeram:   229.427 GiB \n58.7 MB\n  DRS   RSS\n765391 150796\n[1] \"opening dataset\"\nFileSystemDataset with 12 Parquet files\nvendor_name: string\nTrip_Pickup_DateTime: string\nTrip_Dropoff_DateTime: string\nPassenger_Count: int64\nTrip_Distance: double\nStart_Lon: double\nStart_Lat: double\nRate_Code: double\nstore_and_forward: double\nEnd_Lon: double\nEnd_Lat: double\nPayment_Type: string\nFare_Amt: double\nsurcharge: double\nmta_tax: double\nTip_Amt: double\nTolls_Amt: double\nTotal_Amt: double\nmonth: int32\n\nSee $metadata for additional Schema metadata\n[1] \"into lapply\"\n   user  system elapsed \n 59.396   2.090  54.976 \n[1] 10.35707\n[1] \"into mclapply\"\n   user  system elapsed \n 65.569   8.483  50.902 \n[1] 10.35707\n4.336 GiB\nSize:  4.524 GiB \nPeak:  9.906 GiB \n4.67 GB\n[1] 27472535       19"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#exploretlc.r-4-months",
    "href": "materials/lectures/4-data/slides.html#exploretlc.r-4-months",
    "title": "Data",
    "section": "\nexploreTLC.R: 4 months",
    "text": "exploreTLC.R: 4 months\n\n[gostrouc@dt-login03 data]$ cat utk.o\n/u/gostrouc/BZAN_583_code/data\nloaded R\nSize:  146.883 MiB \nPeak:  146.883 MiB \nTotalram:  251.600 GiB \nFreeram:   229.428 GiB \n58.7 MB\n  DRS   RSS\n765395 150660\n[1] \"opening dataset\"\nFileSystemDataset with 12 Parquet files\nvendor_name: string\nTrip_Pickup_DateTime: string\nTrip_Dropoff_DateTime: string\nPassenger_Count: int64\nTrip_Distance: double\nStart_Lon: double\nStart_Lat: double\nRate_Code: double\nstore_and_forward: double\nEnd_Lon: double\nEnd_Lat: double\nPayment_Type: string\nFare_Amt: double\nsurcharge: double\nmta_tax: double\nTip_Amt: double\nTolls_Amt: double\nTotal_Amt: double\nmonth: int32\n\nSee $metadata for additional Schema metadata\n[1] \"into lapply\"\n   user  system elapsed \n127.074   5.567 121.475 \n[1] 10.54913\n[1] \"into mclapply\"\n   user  system elapsed \n135.182  29.250 117.389 \n[1] 10.54913\n8.862 GiB\nSize:   9.553 GiB \nPeak:  22.300 GiB \n9.46 GB\n[1] 56154689       19"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#fread_exploretlc.r-2-months",
    "href": "materials/lectures/4-data/slides.html#fread_exploretlc.r-2-months",
    "title": "Data",
    "section": "\nfread_exploreTLC.R: 2 months",
    "text": "fread_exploreTLC.R: 2 months\n\n[gostrouc@dt-login03 data]$ cat utk.o\n/u/gostrouc/BZAN_583_code/data\nloaded R\nSize:  65.391 MiB \nPeak:  65.391 MiB \nTotalram:  251.600 GiB \nFreeram:   229.182 GiB \n37.3 MB\n  DRS   RSS\n540259 82336\n[1] \"opening dataset\"\n[1] \"into lapply\"\n   user  system elapsed \n 37.783   6.971 515.404 \n[1] 10.35707\n[1] \"into mclapply\"\n   user  system elapsed \n 24.401   6.467  18.516 \n[1] 10.35707\n3.275 GiB\nSize:  3.357 GiB \nPeak:  7.337 GiB \n3.56 GB\n[1] 27472535       18"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#fread_exploretlc.r-4-months",
    "href": "materials/lectures/4-data/slides.html#fread_exploretlc.r-4-months",
    "title": "Data",
    "section": "\nfread_exploreTLC.R: 4 months",
    "text": "fread_exploreTLC.R: 4 months\n\n[gostrouc@dt-login03 data]$ cat utk.o\n/u/gostrouc/BZAN_583_code/data\nloaded R\nSize:  65.430 MiB \nPeak:  65.430 MiB \nTotalram:  251.600 GiB \nFreeram:   229.418 GiB \n37.3 MB\n  DRS   RSS\n540263 82364\n[1] \"opening dataset\"\n[1] \"into mclapply\"\n   user  system elapsed \n 69.074  16.826 216.538 \n[1] 10.54913\n6.694 GiB\nSize:   6.774 GiB \nPeak:  14.178 GiB \n7.23 GB\n[1] 56154689       18\n[gostrouc@dt-login03 data]$"
  },
  {
    "objectID": "materials/lectures/4-data/slides.html#issues-with-slow-data-reads-on-delta",
    "href": "materials/lectures/4-data/slides.html#issues-with-slow-data-reads-on-delta",
    "title": "Data",
    "section": "Issues with Slow Data Reads on Delta",
    "text": "Issues with Slow Data Reads on Delta\n\nCompute nodes are shared if not all cores requested\n\nCan compete for I/O channels with other jobs\n\nTends to be much less busy in AM\n\n\nLustre file system optimized for large binary reads\n\nThat’s /projects and /scratch file systems\nData sets with many files can have issues"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#matrix-libraries",
    "href": "materials/lectures/6-blas/slides.html#matrix-libraries",
    "title": "BLAS Libraries",
    "section": "Matrix Libraries …",
    "text": "Matrix Libraries …"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#r-lapack-blas",
    "href": "materials/lectures/6-blas/slides.html#r-lapack-blas",
    "title": "BLAS Libraries",
    "section": "R-LAPACK-BLAS",
    "text": "R-LAPACK-BLAS\n\nBLAS: Basic Linear Algebra Subroutines - A matrix multiplication library\n\nvector-vector (Level-1), matrix-vector (Level-2), matrix-matrix (Level-3)\n\n\nLAPACK: matrix decompositions and more\n\n\nImplementations: OpenBLAS, Intel MKL, Apple vecLib, AMD BLIS, Nvidia nvBLAS,"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#r-lapack-blas-1",
    "href": "materials/lectures/6-blas/slides.html#r-lapack-blas-1",
    "title": "BLAS Libraries",
    "section": "R-LAPACK-BLAS",
    "text": "R-LAPACK-BLAS\n * BLAS: Basic Linear Algebra Subroutines - A matrix multiplication library\n* vector-vector (Level-1), matrix-vector (Level-2), matrix-matrix (Level-3)\n\n\nLAPACK: dense and banded matrix decompositions and more\n\n\n\\(\\quad LU\\) \\(\\quad LL^T\\) \\(\\quad QR\\) \\(\\quad UDV^T\\) \\(\\quad VD^2V^T\\) \\(\\quad\\|\\cdot\\|_p\\)\n\n\n\nImplementations: OpenBLAS, Intel MKL, Nvidia nvBLAS, Apple vecLib, AMD BLIS, Arm Performance Libraries\nFlexiBLAS: A BLAS and LAPACK wrapper library with runtime exchangable backends"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#r-interfaces-to-low-level-native-tools",
    "href": "materials/lectures/6-blas/slides.html#r-interfaces-to-low-level-native-tools",
    "title": "BLAS Libraries",
    "section": "R Interfaces to Low-Level Native Tools",
    "text": "R Interfaces to Low-Level Native Tools"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#parallelmclapply",
    "href": "materials/lectures/6-blas/slides.html#parallelmclapply",
    "title": "BLAS Libraries",
    "section": "parallel::mclapply()",
    "text": "parallel::mclapply()"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#blas-libraries",
    "href": "materials/lectures/6-blas/slides.html#blas-libraries",
    "title": "BLAS Libraries",
    "section": "BLAS Libraries",
    "text": "BLAS Libraries"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#os-viewpoint-on-blas-libraries",
    "href": "materials/lectures/6-blas/slides.html#os-viewpoint-on-blas-libraries",
    "title": "BLAS Libraries",
    "section": "OS Viewpoint on BLAS Libraries",
    "text": "OS Viewpoint on BLAS Libraries\nFedora Linux OS development cascade:\nFedora \\(\\rightarrow\\) CentOS Stream \\(\\rightarrow\\) Red Hat Enterprise Linux (RHEL)\n\\[\\qquad\\]\nOptimal: BLAS control via FlexiBLAS\n\nR - FlexiBLAS - (NETLIB, OpenBLAS, vecLib, …)\n\nOn Delta, we have spack-installed:\n\nR - OpenBLAS\n\n\nEasyBuild is currently more R-friendly than spack for the HPC BLAS ecosystem\n\nEasyBuild Common toolchains\n\n\n\n\nThese may be relevant for making OS choices when configuring cloud resources"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#set-number-of-threads-for-operation",
    "href": "materials/lectures/6-blas/slides.html#set-number-of-threads-for-operation",
    "title": "BLAS Libraries",
    "section": "Set Number of Threads for Operation",
    "text": "Set Number of Threads for Operation\n\nRhpcBLASctl::omp_set_num_threads()\n\nAlso get information on threads:\n\nRhpcBLASctl::omp_get_num_procs()\nRhpcBLASctl::omp_get_max_threads()"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#examples",
    "href": "materials/lectures/6-blas/slides.html#examples",
    "title": "BLAS Libraries",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/lectures/6-blas/slides.html#flexiblas",
    "href": "materials/lectures/6-blas/slides.html#flexiblas",
    "title": "BLAS Libraries",
    "section": "FlexiBLAS",
    "text": "FlexiBLAS\n\nlibrary(flexiblas)\n\n# check whether FlexiBLAS is available\nflexiblas_avail()\n#&gt; [1] TRUE\n\n# get the current backend\nflexiblas_current_backend()\n#&gt; [1] \"OPENBLAS-OPENMP\"\n\n# list all available backends\nflexiblas_list()\n#&gt; [1] \"NETLIB\"           \"__FALLBACK__\"     \"BLIS-THREADS\"     \"OPENBLAS-OPENMP\"\n#&gt; [5] \"BLIS-SERIAL\"      \"ATLAS\"            \"OPENBLAS-SERIAL\"  \"OPENBLAS-THREADS\"\n#&gt; [9] \"BLIS-OPENMP\"\n\n# get/set the number of threads\nflexiblas_set_num_threads(12)\nflexiblas_get_num_threads()\n#&gt; [1] 12\n\n\n\n\n\n\n\n\n\nhttps://github.com/Enchufa2/r-flexiblashttps://cran.r-project.org/package=flexiblas"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#workflow-for-remote-computing-in-r",
    "href": "materials/lectures/1a-workflow/slides.html#workflow-for-remote-computing-in-r",
    "title": "Workflow for Remote Computing with R",
    "section": "Workflow for Remote Computing in R",
    "text": "Workflow for Remote Computing in R"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#workflow-for-remote-computing-in-r-1",
    "href": "materials/lectures/1a-workflow/slides.html#workflow-for-remote-computing-in-r-1",
    "title": "Workflow for Remote Computing with R",
    "section": "Workflow for Remote Computing in R",
    "text": "Workflow for Remote Computing in R\nLaptop RStudio\n\nFamiliar custom editing environment (Windows, Mac, Unix)\nInteractive Syntax checking\n\ngit, GitHub (or GitLab and Bitbucket)\n\nPortability to remote computing\nVersion control\nCollaboration\n\nCluster unix\n\nSame environment for all\nBatch job submission\n\n\nPortability - in your basement or on another continent\nRStudio installs difficult - legacy OS on HPC\nBandwidth"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#cooperating-r-sessions-on-a-modern-cluster",
    "href": "materials/lectures/1a-workflow/slides.html#cooperating-r-sessions-on-a-modern-cluster",
    "title": "Workflow for Remote Computing with R",
    "section": "Cooperating R Sessions on a Modern Cluster",
    "text": "Cooperating R Sessions on a Modern Cluster\n8 Nodes (124 + 4 cores each)\n- 32 R sessions\n- each using 31 cores\n- 992 cores total\n\nBlue squares are nodes, circles are storage/disk\nInterconnect is communication between nodes\nLaptop - login node - resource script\n\nBIG data on parallel file system - not on laptop!\n\nCan monitor a longer run with logins to compute nodes\n\nBatch"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#software-needed-on-laptop",
    "href": "materials/lectures/1a-workflow/slides.html#software-needed-on-laptop",
    "title": "Workflow for Remote Computing with R",
    "section": "Software Needed on Laptop",
    "text": "Software Needed on Laptop\n\nMac\n\nR, RStudio\nterminal, git (in Xcode)\n\n\nWindows\n\nR, RStudio\nputty\ngit\nWinSCP"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#software-on-cluster",
    "href": "materials/lectures/1a-workflow/slides.html#software-on-cluster",
    "title": "Workflow for Remote Computing with R",
    "section": "Software on Cluster",
    "text": "Software on Cluster\n\nFlexiBLAS, OpenBLAS\nOpenMPI\nArrow or HDF5 (for parallel I/O)\nR (&gt;= 4.0)\n\nAbove managed by center, below by you\n\nAnd various packages, including pbdMPI\n\n\nR vs conda-R Deployment:\n\nDifferent package management philosophy\nCan create conflicts if mixing, a layer of complexity\n\n\nSolves some dependency issues but creates others"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#github-git-laptop-to-cluster",
    "href": "materials/lectures/1a-workflow/slides.html#github-git-laptop-to-cluster",
    "title": "Workflow for Remote Computing with R",
    "section": "GitHub + git (laptop to cluster)",
    "text": "GitHub + git (laptop to cluster)\n\n\n\n\n\n\n\n\n\n\\[\\qquad\\] \n\n\n\n\n\nRStudio demo and checkbox add\n\nnames for head, remote,\n\nGit diagram by Daniel Kinzler - CC BY 3.0"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#making-git-easy",
    "href": "materials/lectures/1a-workflow/slides.html#making-git-easy",
    "title": "Workflow for Remote Computing with R",
    "section": "Making git easy",
    "text": "Making git easy\n\n\nPrivate and Public key pairs for Client and Server\n\nGit repository (GitHub, GitLab) is the server\n\nYour laptop and remote cluster are clients\n\n\n\n\nEach client has own key-pair\n\n\nPrivate key stays on the client\n\n\nPublic key is copied to the server\n\n\n\n\nWorks like a single-use password generator and authentication\n\nClient contacts server, server responds with a random string encrypted by client’s public key\nClient decrypts with own private key and sends to server\nServer verifies agreement and opens secure connection"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#clusters-are-linux-systems",
    "href": "materials/lectures/1a-workflow/slides.html#clusters-are-linux-systems",
    "title": "Workflow for Remote Computing with R",
    "section": "Clusters are Linux systems",
    "text": "Clusters are Linux systems\n\n\nLinux is one of many descendants of original Unix. MacOS is another.\n\nEverything in Linux is a file (some are directory files)\nLinux files are organized as a tree\nEvery file has permissions: d r w x | r w x | r w x\n\nthe first is a directory bit\nthe triples are read, write, execute\nthe grouping is owner, group, all\n\n\nEvery file has owner, group, and a few other attributes\nAvailable commands are executable files in your PATH directories\nPATH value can be seen with echo $PATH\n\n\n\n\nUnix (Bell Labs) was a play on an earlier Multics operating system (designed at MIT)"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#linux-commands",
    "href": "materials/lectures/1a-workflow/slides.html#linux-commands",
    "title": "Workflow for Remote Computing with R",
    "section": "Linux Commands",
    "text": "Linux Commands\nIn a terminal, you talk to a shell program (bash is most common)\n\nCommands (executable files) can have options and arguments\n\n\nStandard input and standard output of a command is the terminal but can be redirected\n\n\n&lt;, &lt;&lt;, &gt;, &gt;&gt; redirect standard input and output\n\ncommand1 | command2 pipes standard output1 to standard input2\n\nCommands are files in directories listed in your PATH variable (try “echo $PATH”)\n\n$ means substitute variable value\n\nexport lists (or sets) variables and their values\n\nThere are many resources on the web to learn Linux basics"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#solutions-for-potential-windows-issues",
    "href": "materials/lectures/1a-workflow/slides.html#solutions-for-potential-windows-issues",
    "title": "Workflow for Remote Computing with R",
    "section": "Solutions for Potential Windows Issues",
    "text": "Solutions for Potential Windows Issues\n\nFiles produced in text editors may have Windows line endings\n\nWindows: \\r\\n, Unix: \\n\n\nFix on Unix command line: sed -i.bak 's/\\r$//' file.txt\n\nEdits in-place and creates a backup file with .bak extension\n\n\n\n\nRStudio on Windows may not recognize Unix line endings\n\nChange in RStudio: Tools -&gt; Global Options -&gt; Code -&gt; Line endings"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#use-of-login-nodes-and-compute-nodes",
    "href": "materials/lectures/1a-workflow/slides.html#use-of-login-nodes-and-compute-nodes",
    "title": "Workflow for Remote Computing with R",
    "section": "Use of Login Nodes and Compute Nodes",
    "text": "Use of Login Nodes and Compute Nodes\nLogin nodes:\n\nCode downloads, GitHub or other\nData downloads from Internet\nNo RStudio\n\nInteractive R or Rscript sessions:\n\nPackage installs\n\nInteractive debugging\n\nRscript debugging\n\n\n\nCompute nodes:\n- No Internet access\n- No RStudio\n- Batch Rscript submission via Slurm\n\nBlue squares are nodes, circles are storage/disk\nInterconnect is communication between nodes\nLaptop - login node - resource script\n\nBIG data on parallel file system - not on laptop!\n\nCan monitor a longer run with logins to compute nodes\n\nBatch"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#on-a-cluster-login-node",
    "href": "materials/lectures/1a-workflow/slides.html#on-a-cluster-login-node",
    "title": "Workflow for Remote Computing with R",
    "section": "On a Cluster Login Node",
    "text": "On a Cluster Login Node\nYou are interacting with a bash shell ($ prompt)\nSubmit batch scripts to the compute nodes (next slide)\nTp run R on the login node, load its module\n\n$ module load r\n\nThen you can run of your script.R file with\n\n$ Rscript script.R\n\nOutput goes to the terminal but you can also redirect it to a file\n\n$ Rscript script.R &gt; outfile_name\n\nOr start an interactive R session with\n\n$ R\n\nwhich will give you a &gt; prompt. You quit the R session with\n&gt; q()\nand logout with\n\n$ exit\n\n\n\nPrompts $ and &gt; are not part of the shown commands"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#submit-scripts-to-the-compute-nodes",
    "href": "materials/lectures/1a-workflow/slides.html#submit-scripts-to-the-compute-nodes",
    "title": "Workflow for Remote Computing with R",
    "section": "Submit scripts to the compute nodes",
    "text": "Submit scripts to the compute nodes\nTo run your R script on one or more compute nodes, you need two files\n\nThe R script, say script.R\n\nA bash shell script that requests the nodes, cores, etc. and specifies how to run your script.R, whis is usually Rscript script.R.\n\nHere is an example shell script to run script.R\n\n#!/bin/bash\n#SBATCH --job-name utk\n#SBATCH --account=bckj-delta-cpu\n#SBATCH --partition=cpu\n#SBATCH --nodes=1\n#SBATCH --mem=16g\n#SBATCH --cpus-per-task=1\n#SBATCH --time 00:00:30\n#SBATCH -e ./utk.e\n#SBATCH -o ./utk.o\n\nmodule load r\nRscript script.R\n\nrequesting one core on one node, limit of 30 seconds. If this file is saved as script.sh\n\nsbatch script.sh\n\nsubmits the job. Check on its status with\n\nsqueue -u &lt;your-user-name&gt;"
  },
  {
    "objectID": "materials/lectures/1a-workflow/slides.html#package-installation-on-cluster",
    "href": "materials/lectures/1a-workflow/slides.html#package-installation-on-cluster",
    "title": "Workflow for Remote Computing with R",
    "section": "Package Installation on Cluster",
    "text": "Package Installation on Cluster\nInstall packages in an interactive R session on a login node\nLogin to the cluster, then at the shell prompt:\n\nmodule load r\nR\n\nstarts an interactive R session. Now, for example, to install package dplyr:\ninstall.packages(\"dplyr\", repo = \"https://cloud.r-project.org/\")\nThe repo parameter can be skipped if it is set in options(). R may also ask to choose a repository location.\nYou may need to load other modules if a package has external to R dependencies.\n\nmodule avail\n\nwill list available software modules. If you don’t see the software, ask help@ncsa.illinois.edu to have it installed. Note that this is for external to R software (not R packages)."
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#two-three-basic-concepts-in-hardware",
    "href": "materials/lectures/3-software/slides.html#two-three-basic-concepts-in-hardware",
    "title": "HPC Software",
    "section": "Two (Three) Basic Concepts in Hardware",
    "text": "Two (Three) Basic Concepts in Hardware"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#two-three-basic-concepts-from-software-viewpoint",
    "href": "materials/lectures/3-software/slides.html#two-three-basic-concepts-from-software-viewpoint",
    "title": "HPC Software",
    "section": "(Two) Three Basic Concepts from Software Viewpoint",
    "text": "(Two) Three Basic Concepts from Software Viewpoint"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#native-programming-mindset",
    "href": "materials/lectures/3-software/slides.html#native-programming-mindset",
    "title": "HPC Software",
    "section": "Native Programming Mindset",
    "text": "Native Programming Mindset"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#native-programming-models-and-tools",
    "href": "materials/lectures/3-software/slides.html#native-programming-models-and-tools",
    "title": "HPC Software",
    "section": "Native Programming Models and Tools",
    "text": "Native Programming Models and Tools"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#years-of-parallel-computing",
    "href": "materials/lectures/3-software/slides.html#years-of-parallel-computing",
    "title": "HPC Software",
    "section": "40 Years of Parallel Computing",
    "text": "40 Years of Parallel Computing"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#last-20-years-of-advances",
    "href": "materials/lectures/3-software/slides.html#last-20-years-of-advances",
    "title": "HPC Software",
    "section": "Last 20 years of Advances",
    "text": "Last 20 years of Advances"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#distributed-programming-works-in-shared-memory",
    "href": "materials/lectures/3-software/slides.html#distributed-programming-works-in-shared-memory",
    "title": "HPC Software",
    "section": "Distributed Programming Works in Shared Memory",
    "text": "Distributed Programming Works in Shared Memory"
  },
  {
    "objectID": "materials/lectures/3-software/slides.html#r-interfaces-to-low-level-native-tools",
    "href": "materials/lectures/3-software/slides.html#r-interfaces-to-low-level-native-tools",
    "title": "HPC Software",
    "section": "R Interfaces to Low-Level Native Tools",
    "text": "R Interfaces to Low-Level Native Tools"
  },
  {
    "objectID": "materials/lectures/4-parallel/slides.html#embarrassingly-pleasingly-parallel",
    "href": "materials/lectures/4-parallel/slides.html#embarrassingly-pleasingly-parallel",
    "title": "Measurement and terminology of parallel speedup",
    "section": "Embarrassingly (Pleasingly) Parallel",
    "text": "Embarrassingly (Pleasingly) Parallel\n\\[\\qquad\\]\n- \\[t_p = \\frac{t}{n} = t_n\\] - \\[\\mbox{Speedup:}\\quad\\frac{t}{t_p} = n\\] \\(t\\) - Serial time\\(n\\) - Number of chunks (or processes)\\(t_n\\) - Single chunk time with \\(n\\) chunks\\(t_p\\) - Parallel time\nBest case scenario: no overhead, no communication"
  },
  {
    "objectID": "materials/lectures/4-parallel/slides.html#serial-section-amdahls-law",
    "href": "materials/lectures/4-parallel/slides.html#serial-section-amdahls-law",
    "title": "Measurement and terminology of parallel speedup",
    "section": "Serial Section (Amdahl’s Law)",
    "text": "Serial Section (Amdahl’s Law)\n\\[\\qquad\\] \\[\\qquad\\]\\[t_p = t_s + t_n &gt; t_s\\]\\[\\mbox{Maximum Speedup:}\\quad\\lim_{n \\to \\infty}\\frac{t}{t_p} = \\frac{t}{t_s}\\]\n\\(t\\) - Serial time (fixed)\\(n\\) - Number of chunks (or processes)\\(t_n\\) - single chunk time with \\(n\\) chunks\\(t_p\\) - Parallel time\\(t_s\\) - Serial section time\nStrong Scaling: fixed work, increasing resources"
  },
  {
    "objectID": "materials/lectures/4-parallel/slides.html#serial-section-gustafsons-law",
    "href": "materials/lectures/4-parallel/slides.html#serial-section-gustafsons-law",
    "title": "Measurement and terminology of parallel speedup",
    "section": "Serial Section (Gustafson’s Law)",
    "text": "Serial Section (Gustafson’s Law)\n\\[\\qquad\\]\\[\\qquad\\] \\[t_p = t_s + t_n\\]\\[\\mbox{Speedup:}\\quad\\frac{t}{t_p} = \\frac{t_s + nt_n}{t_s + t_n} = O(n)\\]\n\\(t\\) - Serial time (growing: \\(t_{2n} = 2t_n\\) )\\(n\\) - Number of chunks (or processes)\\(t_n\\) - single chunk time with \\(n\\) chunks\\(t_p\\) - Parallel time\\(t_s\\) - Serial section time\nWeak Scaling: increasing work, increasing resources\nWeak: misnomer: the speedup is actually great"
  },
  {
    "objectID": "materials/lectures/4-parallel/slides.html#parallel-overhead",
    "href": "materials/lectures/4-parallel/slides.html#parallel-overhead",
    "title": "Measurement and terminology of parallel speedup",
    "section": "Parallel Overhead",
    "text": "Parallel Overhead\n\\[\\qquad\\]\\[\\qquad\\]\\[t_p = t_s + t_n + t_o(n)\\]\\(t\\) - Serial time\\(n\\) - Number of processes\\(t_n\\) - single chunk time with \\(n\\) chunks\\(t_p\\) - Parallel time\\(t_s\\) - Serial section time\\(t_o(n)\\) - Parallel overhead time"
  },
  {
    "objectID": "materials/lectures/4-parallel/slides.html#replication-of-serial-section-distirbuted",
    "href": "materials/lectures/4-parallel/slides.html#replication-of-serial-section-distirbuted",
    "title": "Measurement and terminology of parallel speedup",
    "section": "Replication of Serial Section (Distirbuted)",
    "text": "Replication of Serial Section (Distirbuted)\n\\[\\qquad\\]\\[\\qquad\\] \\[\\qquad\\] \\[\\qquad\\]\n\\(t\\) - Serial time\\(n\\) - Number of processes\\(t_n\\) - single chunk time with \\(n\\) chunks\\(t_p\\) - Parallel time\\(t_s\\) - Serial section time\\(t_o(n)\\) - Parallel overhead time\nReplication can reduce communication overhead due to communication"
  },
  {
    "objectID": "materials/lectures/8-presentations/slides.html#lightning-talk-1",
    "href": "materials/lectures/8-presentations/slides.html#lightning-talk-1",
    "title": "Untitled",
    "section": "Lightning Talk 1",
    "text": "Lightning Talk 1"
  },
  {
    "objectID": "materials/lectures/8-presentations/slides.html#lightning-talk-2",
    "href": "materials/lectures/8-presentations/slides.html#lightning-talk-2",
    "title": "Untitled",
    "section": "Lightning Talk 2",
    "text": "Lightning Talk 2"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#distributed-memory-computing",
    "href": "materials/lectures/7-distributed/slides.html#distributed-memory-computing",
    "title": "Distributed Computing",
    "section": "Distributed Memory Computing",
    "text": "Distributed Memory Computing"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#working-with-a-remote-cluster-using-r",
    "href": "materials/lectures/7-distributed/slides.html#working-with-a-remote-cluster-using-r",
    "title": "Distributed Computing",
    "section": "Working with a remote cluster using R",
    "text": "Working with a remote cluster using R"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#running-distributed-on-a-cluster",
    "href": "materials/lectures/7-distributed/slides.html#running-distributed-on-a-cluster",
    "title": "Distributed Computing",
    "section": "Running Distributed on a Cluster",
    "text": "Running Distributed on a Cluster"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#parallel-r-tools",
    "href": "materials/lectures/7-distributed/slides.html#parallel-r-tools",
    "title": "Distributed Computing",
    "section": "Parallel R Tools",
    "text": "Parallel R Tools"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#distributed-memory-tools",
    "href": "materials/lectures/7-distributed/slides.html#distributed-memory-tools",
    "title": "Distributed Computing",
    "section": "Distributed Memory Tools",
    "text": "Distributed Memory Tools"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#message-passing-interface-mpi",
    "href": "materials/lectures/7-distributed/slides.html#message-passing-interface-mpi",
    "title": "Distributed Computing",
    "section": "Message Passing Interface (MPI)",
    "text": "Message Passing Interface (MPI)"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#single-program-multiple-data-spmd",
    "href": "materials/lectures/7-distributed/slides.html#single-program-multiple-data-spmd",
    "title": "Distributed Computing",
    "section": "Single Program Multiple Data (SPMD)",
    "text": "Single Program Multiple Data (SPMD)\n\nN instances of the same code cooperate\n\nEach of the N instances has rank, {0, . . ., N-1}\nThe rank determines any differences in work\nInstances run asynchronously\n\nSPMD parallelization is a generalization of the serial code\n\nMany rank-aware operations are automated\nCollective operations are high level and easy to learn\nExplicit point-to-point communications are an advanced topic\nMultilevel parallelism is possible\n\nTypically no manager, it is all cooperation"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#running-mpi-on-a-laptop",
    "href": "materials/lectures/7-distributed/slides.html#running-mpi-on-a-laptop",
    "title": "Distributed Computing",
    "section": "Running MPI on a Laptop",
    "text": "Running MPI on a Laptop\nmacOS in a Terminal window:\n\nbrew install openmpi\nIn an R session: install.packages(\"pbdMPI\")\nmpirun -np 4 Rscript your_spmd_code.R\n\nWindows\n\nWeb Page: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\nDownload: https://www.microsoft.com/en-us/download/details.aspx?id=100593\npbdMPI has a Windows binary on CRAN"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#mpi-hello-world",
    "href": "materials/lectures/7-distributed/slides.html#mpi-hello-world",
    "title": "Distributed Computing",
    "section": "MPI Hello World",
    "text": "MPI Hello World\nSPMD: \\(n\\) identical codes running, each given a unique rank: \\(0\\) to \\(n-1\\)\nhello_world.R\nsuppressMessages(library(pbdMPI))\n\nmy_rank = comm.rank()\nnranks = comm.size()\nmsg = paste0(\"Hello World! My name is Rank\", my_rank, \". We are \", nranks, \" identical siblings.\")\ncat(msg, \"\\n\")\n\nfinalize()"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#mpi-hello-world-1",
    "href": "materials/lectures/7-distributed/slides.html#mpi-hello-world-1",
    "title": "Distributed Computing",
    "section": "MPI Hello World",
    "text": "MPI Hello World\nShell script to run 8 instances of hello_world.R\nhello_world.sh\n#!/bin/bash\n#SBATCH --job-name utk\n#SBATCH --account=bckj-delta-cpu\n#SBATCH --partition=cpu\n#SBATCH --mem=20g\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=8\n#SBATCH --tasks-per-node=8\n#SBATCH --time 00:10:00\n#SBATCH -e ./utk.e\n#SBATCH -o ./utk.o\n\npwd\nmodule load r\nmodule list\n\ntime mpirun -np 8 Rscript hello_world.R\n\nSubmit with: sbatch hello_world.sh"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#mpi-output-utk.o",
    "href": "materials/lectures/7-distributed/slides.html#mpi-output-utk.o",
    "title": "Distributed Computing",
    "section": "MPI Output utk.o",
    "text": "MPI Output utk.o\n[gostrouc@dt-login04 mpi]$ cat utk.o\n/u/gostrouc/BZAN_583_code/mpi\n\n======================   ALLOCATED NODES   ======================\n    cn095: flags=0x11 slots=8 max_slots=0 slots_inuse=0 state=UP\n=================================================================\nHello World! My name is Rank1. We are 8 identical siblings. \nHello World! My name is Rank0. We are 8 identical siblings. \nHello World! My name is Rank4. We are 8 identical siblings. \nHello World! My name is Rank5. We are 8 identical siblings. \nHello World! My name is Rank6. We are 8 identical siblings. \nHello World! My name is Rank2. We are 8 identical siblings. \nHello World! My name is Rank3. We are 8 identical siblings. \nHello World! My name is Rank7. We are 8 identical siblings. \n\nNote the rank order is arbitrary\n\nIn general, cat() does not even prevent overprinting!"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#mpi-hello-world-2",
    "href": "materials/lectures/7-distributed/slides.html#mpi-hello-world-2",
    "title": "Distributed Computing",
    "section": "MPI Hello World",
    "text": "MPI Hello World\nSPMD: some control over printing with comm.cat()\nhello_world2.R\nsuppressMessages(library(pbdMPI))\n\nmy_rank = comm.rank()\nnranks = comm.size()\nmsg = paste0(\"Hello World! My name is Rank\", my_rank, \". We are \", nranks, \" identical siblings.\")\ncomm.cat(msg, \"\\n\", all.rank = TRUE)\n\nfinalize()"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#mpi-output-utk.o-1",
    "href": "materials/lectures/7-distributed/slides.html#mpi-output-utk.o-1",
    "title": "Distributed Computing",
    "section": "MPI Output utk.o",
    "text": "MPI Output utk.o\n[gostrouc@dt-login04 mpi]$ cat utk.o\n/u/gostrouc/BZAN_583_code/mpi\n\n======================   ALLOCATED NODES   ======================\n    cn095: flags=0x11 slots=8 max_slots=0 slots_inuse=0 state=UP\n=================================================================\nCOMM.RANK = 0\nHello World! My name is Rank0. We are 8 identical siblings. \nCOMM.RANK = 1\nHello World! My name is Rank1. We are 8 identical siblings. \nCOMM.RANK = 2\nHello World! My name is Rank2. We are 8 identical siblings. \nCOMM.RANK = 3\nHello World! My name is Rank3. We are 8 identical siblings. \nCOMM.RANK = 4\nHello World! My name is Rank4. We are 8 identical siblings. \nCOMM.RANK = 5\nHello World! My name is Rank5. We are 8 identical siblings. \nCOMM.RANK = 6\nHello World! My name is Rank6. We are 8 identical siblings. \nCOMM.RANK = 7\nHello World! My name is Rank7. We are 8 identical siblings."
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#two-parallel-levels",
    "href": "materials/lectures/7-distributed/slides.html#two-parallel-levels",
    "title": "Distributed Computing",
    "section": "Two Parallel Levels",
    "text": "Two Parallel Levels\nhello_balance.R\n## This script describes two levels of parallelism:\n## Top level: Distributed MPI runs several copies of this entire script. Instances differ by their comm.rank() designation.\n## Inner level: The unix fork (copy-on-write) shared memory parallel execution of the mc.function() managed by parallel::mclapply()\n## Further levels are possible: multithreading in compiled code and communicator splitting at the distributed MPI level.\n\nsuppressMessages(library(pbdMPI))\ncomm.print(sessionInfo())\n\nhost = system(\"hostname\", intern = TRUE)  # get node name\n\nmc.function = function(x) {\n    Sys.sleep(0.1) # replace with your function for mclapply cores here\n    Sys.getpid() # returns process id\n}\n\n## ranks and cores queries\nranks_on_my_node = Sys.getenv(\"SLURM_NTASKS_PER_NODE\")\nmy_cores = Sys.getenv(\"SLURM_CPUS_PER_TASK\")\ncores_on_my_node = Sys.getenv(\"SLURM_CPUS_ON_NODE\")\ncores_total = allreduce(my_cores)  # adds up over ranks\n\n## Run mclapply on allocated cores to demonstrate fork pids\nmy_pids = parallel::mclapply(seq_len(my_cores), mc.function, mc.cores = my_cores)\nmy_pids = do.call(paste, my_pids) # combines results from mclapply\n##\n## Same cores are shared with OpenBLAS (see flexiblas package) or for other OpenMP enabled codes outside mclapply.\n## If BLAS functions are called inside mclapply, they compete for the same cores: avoid or manage appropriately!!!\n\n## Now report what happened and where\nmsg = paste0(\"Hello World from rank \", comm.rank(), \" on host \", host, \" with \", my_cores, \" cores allocated\\n\",\n             \"            (\", ranks_on_my_node, \" R sessions sharing \", cores_on_my_node, \" cores on this host node).\\n\",\n             \"      pid: \", my_pids, \"\\n\")\ncomm.cat(msg, all.rank = TRUE)\n\nbarrier()  # wait for all ranks to reach this point\ncomm.cat(\"Total R sessions:\", comm.size(), \"Total cores:\", cores_total, \"\\n\", quiet = TRUE)\ncomm.cat(\"\\nNotes: pid to core map changes frequently during mclapply\\n\", quiet = TRUE)\n\nfinalize()"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#two-parallel-levels-output",
    "href": "materials/lectures/7-distributed/slides.html#two-parallel-levels-output",
    "title": "Distributed Computing",
    "section": "Two Parallel Levels Output",
    "text": "Two Parallel Levels Output\n[gostrouc@dt-login04 mpi]$ cat utk.o\n/u/gostrouc/BZAN_583_code/mpi\n\n======================   ALLOCATED NODES   ======================\n    cn096: flags=0x11 slots=4 max_slots=0 slots_inuse=0 state=UP\n    cn099: flags=0x10 slots=4 max_slots=0 slots_inuse=0 state=UP\n=================================================================\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Red Hat Enterprise Linux 8.8 (Ootpa)\n\nMatrix products: default\nBLAS/LAPACK: /sw/spack/deltas11-2023-03/apps/linux-rhel8-zen3/gcc-11.4.0/openblas-0.3.25-5yvxjnl/lib/libopenblas_zen-r0.3.25.so;  LAPACK version 3.11.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] pbdMPI_0.5-1\n\nloaded via a namespace (and not attached):\n[1] compiler_4.3.2 parallel_4.3.2 float_0.3-2   \nCOMM.RANK = 0\nHello World from rank 0 on host cn096.delta.ncsa.illinois.edu with 6 cores allocated\n            (4 R sessions sharing 24 cores on this host node).\n      pid: 939702 939706 939708 939712 939715 939719\nCOMM.RANK = 1\nHello World from rank 1 on host cn096.delta.ncsa.illinois.edu with 6 cores allocated\n            (4 R sessions sharing 24 cores on this host node).\n      pid: 939704 939709 939714 939718 939722 939723\nCOMM.RANK = 2\nHello World from rank 2 on host cn096.delta.ncsa.illinois.edu with 6 cores allocated\n            (4 R sessions sharing 24 cores on this host node).\n      pid: 939703 939707 939710 939713 939716 939720\nCOMM.RANK = 3\nHello World from rank 3 on host cn096.delta.ncsa.illinois.edu with 6 cores allocated\n            (4 R sessions sharing 24 cores on this host node).\n      pid: 939705 939711 939717 939721 939724 939725\nCOMM.RANK = 4\nHello World from rank 4 on host cn099.delta.ncsa.illinois.edu with 6 cores allocated\n            (1 R sessions sharing 24 cores on this host node).\n      pid: 1152806 1152810 1152816 1152821 1152824 1152826\nCOMM.RANK = 5\nHello World from rank 5 on host cn099.delta.ncsa.illinois.edu with 6 cores allocated\n            (1 R sessions sharing 24 cores on this host node).\n      pid: 1152804 1152808 1152812 1152814 1152818 1152822\nCOMM.RANK = 6\nHello World from rank 6 on host cn099.delta.ncsa.illinois.edu with 6 cores allocated\n            (1 R sessions sharing 24 cores on this host node).\n      pid: 1152803 1152807 1152811 1152813 1152817 1152819\nCOMM.RANK = 7\nHello World from rank 7 on host cn099.delta.ncsa.illinois.edu with 6 cores allocated\n            (1 R sessions sharing 24 cores on this host node).\n      pid: 1152805 1152809 1152815 1152820 1152823 1152825\nTotal R sessions: 8 Total cores: 48 \n\nNotes: pid to core map changes frequently during mclapply"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#pbdr-project",
    "href": "materials/lectures/7-distributed/slides.html#pbdr-project",
    "title": "Distributed Computing",
    "section": "pbdR Project",
    "text": "pbdR Project\n\n\nBridge HPC with high-productivity of R: Expressive for data and modern statistics\nKeep syntax identical to R, when possible\nSoftware reuse philosophy:\n\nDon’t reinvent the wheel when possible\nIntroduce HPC standards with R flavor\nUse scalable HPC libraries with R convenience\n\nSimplify and use R intelligence where possible\n\n\nUsing HPC concepts and libraries * Benefits the R user by knowing standard components of HPC"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#package-pbdmpi",
    "href": "materials/lectures/7-distributed/slides.html#package-pbdmpi",
    "title": "Distributed Computing",
    "section": "Package pbdMPI",
    "text": "Package pbdMPI\n\nSpecializes in SPMD programming for HPC clusters\n\nManages printing from ranks\nProvides chunking options\nProvides communicator splits for multilevel parallelism\nIn situ capability to process data from other MPI codes without copy\n\nA derivation and rethinking of the Rmpi package aimed at HPC clusters\n\nSimplified interface with fewer parameters (using R’s S4 methods)\nFaster for matrix and array data - no serialization\n\n\n\n\nPrefer pbdMPI over Rmpi due to simplification and speed\n\nNo serialization for arrays and vectors\n\nDrops spawning a cluster\n\nBecause a client-server relationship is more appropriate"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#pbdmpi-high-level-collective-communications",
    "href": "materials/lectures/7-distributed/slides.html#pbdmpi-high-level-collective-communications",
    "title": "Distributed Computing",
    "section": "pbdMPI: High-level Collective Communications",
    "text": "pbdMPI: High-level Collective Communications\nEach of these operations is performed across a communicator of ranks. Simplest one is all ranks but rank arrays can be used for multilevel collectives.\n\nreduce() Reduces a set of same-size distributed vectors or arrays with an operation (+ is default). Fast because both communication and reduction are parallel and no serialization is needed.\nallreduce() Same as reduce() except all ranks in a comm get the result\ngather() Gathers a set of distributed objects\nallgather() Same as gather() except all ranks in a comm get the result\nbcast() Broadcasts an object from one rank to all in its comm\nscatter() Broadcasts different pieces of an object from one rank to all in its comm\nbarrier() Waits on all ranks in a comm before proceeding"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#pbdmpi-high-level-collective-operations",
    "href": "materials/lectures/7-distributed/slides.html#pbdmpi-high-level-collective-operations",
    "title": "Distributed Computing",
    "section": "pbdMPI: High-level Collective Operations",
    "text": "pbdMPI: High-level Collective Operations\n\\(\\small \\bf A = \\sum_{i=1}^nX_i\\) \\(\\quad\\) \\(\\qquad\\) \\(\\qquad\\) A = reduce(X) \\(\\qquad\\) \\(\\qquad\\) A = allreduce(X)\n\\(\\small \\bf A = \\left[ X_1 | X_2 | \\cdots | X_n \\right]\\) \\(\\qquad\\) A = gather(X) \\(\\qquad\\) \\(\\qquad\\) A = allgather(X)\n\n\n\nPowerful: communication and reduction is highly parallel\n\nthat’s why it beats Spark/MapReduce"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#pbdmpi-functions-to-facilitate-spmd-programming",
    "href": "materials/lectures/7-distributed/slides.html#pbdmpi-functions-to-facilitate-spmd-programming",
    "title": "Distributed Computing",
    "section": "pbdMPI: Functions to Facilitate SPMD Programming",
    "text": "pbdMPI: Functions to Facilitate SPMD Programming\n\ncomm.chunk() splits a number into chunks in various ways and various formats. Tailored for SPMD programming, returning rank-specific results.\ncomm.set.seed() sets the seed of a parallel RNG. If diff = FALSE, then all ranks generate the same stream. Otherwise, ranks generate different streams.\ncomm.print() and comm.cat() print by default from rank 0 only, with options to print from any or all ranks."
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#distributed-programming-works-in-shared-memory",
    "href": "materials/lectures/7-distributed/slides.html#distributed-programming-works-in-shared-memory",
    "title": "Distributed Computing",
    "section": "Distributed Programming Works in Shared Memory",
    "text": "Distributed Programming Works in Shared Memory"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#random-forest-with-mpi",
    "href": "materials/lectures/7-distributed/slides.html#random-forest-with-mpi",
    "title": "Distributed Computing",
    "section": "Random Forest with MPI",
    "text": "Random Forest with MPI\nmpi/rf_mpi.R\nsuppressPackageStartupMessages(library(randomForest))\ndata(LetterRecognition, package = \"mlbench\")\nlibrary(pbdMPI, quiet = TRUE)                #&lt;&lt;\ncomm.set.seed(seed = 7654321, diff = FALSE)      #&lt;&lt;\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ][comm.chunk(n_test, form = \"vector\"), ]    #&lt;&lt;\n\ncomm.set.seed(seed  = 1234, diff = TRUE)          #&lt;&lt;\nmy.rf = randomForest(lettr ~ ., train, ntree = comm.chunk(500), norm.votes = FALSE) #&lt;&lt;\nrf.all = allgather(my.rf)                  #&lt;&lt;\nrf.all = do.call(combine, rf.all)          #&lt;&lt;\npred = as.vector(predict(rf.all, test))\n\ncorrect = allreduce(sum(pred == test$lettr))  #&lt;&lt;\ncomm.cat(\"Proportion Correct:\", correct/(n_test), \"\\n\")\n\nfinalize()          #&lt;&lt;"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#comm.chunk",
    "href": "materials/lectures/7-distributed/slides.html#comm.chunk",
    "title": "Distributed Computing",
    "section": "comm.chunk()",
    "text": "comm.chunk()\nmpi/chunk.r\nlibrary( pbdMPI, quiet = TRUE )\n\nmy.rank = comm.rank( )\n\nk = comm.chunk( 10 )\ncomm.cat( my.rank, \":\", k, \"\\n\", all.rank = TRUE, quiet = TRUE)\n\nk = comm.chunk( 10 , form = \"vector\")\ncomm.cat( my.rank, \":\", k, \"\\n\", all.rank = TRUE, quiet = TRUE)\n\nk = comm.chunk( 10 , form = \"vector\", type = \"equal\")\ncomm.cat( my.rank, \":\", k, \"\\n\", all.rank = TRUE, quiet = TRUE)\n\nfinalize( )"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#other-short-mpi-codes",
    "href": "materials/lectures/7-distributed/slides.html#other-short-mpi-codes",
    "title": "Distributed Computing",
    "section": "Other short MPI codes",
    "text": "Other short MPI codes\nbcast.r chunk.r comm_split.R cov.r gather-named.r gather.r gather-unequal.r hello-p.r hello.r map-reduce.r mcsim.r ols.r qr-cop.r rank.r reduce-mat.r timer.r\n\nThese short codes only use pbdMPI and can run on a laptop in a terminal window if you installed OpenMPI\nOn the clusters these can run on a login node with a small \\(^*\\) number of ranks\nWile in the mpi_shorts directory, run the following\n\nsource ../code_4/modules_MACHINE.sh\nmpirun -np 4 Rscript your_script.r"
  },
  {
    "objectID": "materials/lectures/7-distributed/slides.html#shared-memory---mpi-or-fork",
    "href": "materials/lectures/7-distributed/slides.html#shared-memory---mpi-or-fork",
    "title": "Distributed Computing",
    "section": "Shared Memory - MPI or fork?",
    "text": "Shared Memory - MPI or fork?\n\n\n\n\nfork via mclapply() + do.call()\n\n\nMPI replicated data + allreduce()\n\n\nMPI chunked data + allreduce()\n\n\n\n\ndo.call() is serial\nallreduce() is parallel"
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#github-copilot",
    "href": "materials/lectures/6-nn/slides.html#github-copilot",
    "title": "LLMs and torch",
    "section": "GitHub Copilot",
    "text": "GitHub Copilot\nRStudio now has a GitHub Copilot option\nFree to students and academics ($20/month otherwise - says Copilot, but I think it’s $10)\n\n\nThis slide is being written with Copilot on\nIt’s a bit like a smart autocomplete\nAbove, it suggested a non-existent link for the GitHub Copilot option (now fixed)\nIt’s better than autocomplete in RStudio\nBut I find it unhelpful for HPC facing R code\nIt does a little better with shell scripts\n\n\nA video and conference talk\n\n\nExperiments with GPT 3.5\nR is mostly laptop\n\nparallel suggestions are mostly laptop oriented\n\nPlanned demo with rf-serial into rf_mc\n\nfailed to get it to work"
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#torch-pytorch-and-torch",
    "href": "materials/lectures/6-nn/slides.html#torch-pytorch-and-torch",
    "title": "LLMs and torch",
    "section": "Torch, Pytorch, and torch",
    "text": "Torch, Pytorch, and torch\n\nTorch is a C++ library that provides neural network and optimization routines that work with tensors (multi-dimensional arrays)\n\n\n\nPytorch is a deep learning library for Python that is based on the Torch library\ntorch is a package for R that provides an interface to the Torch library\n\nTakes about 15 minutes to install on Delta\n\nInstalls in a standard interactive R session on a login node\n\nCopilot: I have a video on how to install it\nCopilot: Copilot is wrong, but I do have a video on how to install it (Not!)"
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#torch-via-torch-in-r-resources",
    "href": "materials/lectures/6-nn/slides.html#torch-via-torch-in-r-resources",
    "title": "LLMs and torch",
    "section": "Torch via torch in R Resources",
    "text": "Torch via torch in R Resources\n\nA book: Deep Learning and Scientific Computing with R torch\n\n\n\nAccompanying GitHub repo, more recent from mlverse\nA blog, another blog\nTorch Hugging Face Integrations and mlverse\nPosit AI blogs"
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#torch-via-torch-in-r-overview",
    "href": "materials/lectures/6-nn/slides.html#torch-via-torch-in-r-overview",
    "title": "LLMs and torch",
    "section": "Torch via torch in R Overview",
    "text": "Torch via torch in R Overview\n\nA tensor is a multi-dimensional array (vector, matrix, etc.)\nTensor types in torch are: float, double, int, long, byte, short, char, bool, complex, and about 60 others, created with torch_&lt;type&gt;()\n\nLess memory overhead than R’s double\n\nR stores only a reference to the tensor in the R object\ntorch is object-oriented, as opposed to functional in R\n\nFewer copy operations than in R generally\nImplemented with R7 classes (see talk)\n\ntorch is lazy like R, so you may need to call torch$run() to execute\ntorch can put objects on CPU or GPU\ntorch provides functions primarily for neural networks, but also for optimization, and linear algebra"
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#torch-via-torch-in-r-examples",
    "href": "materials/lectures/6-nn/slides.html#torch-via-torch-in-r-examples",
    "title": "LLMs and torch",
    "section": "Torch via torch in R Examples",
    "text": "Torch via torch in R Examples\nFollowing the original blog by Sigrid Keydana, the example code is on GitHub in HPCinR/BZAN_583_code repository in the torch directory:\n\nR script: torch_example_gpu.R\nBash script: torch_example_gpu.sh\n\nThe code uses the Kuzushiji-MNIST (Clanuwat et al. 2018) data (70,000 images 28x28 of 10 characters), considered an “MNIST drop-in”. Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution 28x28."
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#everyone-is-now-added-to-the-gpu-allocation-on-delta-bckj-delta-gpu",
    "href": "materials/lectures/6-nn/slides.html#everyone-is-now-added-to-the-gpu-allocation-on-delta-bckj-delta-gpu",
    "title": "LLMs and torch",
    "section": "Everyone is now added to the GPU allocation on Delta: bckj-delta-gpu",
    "text": "Everyone is now added to the GPU allocation on Delta: bckj-delta-gpu\n\nNVIDIA A40 GPUs\nQueue times are longer than on the CPU nodes (15-30 minutes)\nInstalling torch: in a login node R session, may take two cycles\nInstalling torchvision: needs module load jpeg and possibly\n\nexport LD_LIBRARY_PATH=/sw/spack/deltas11-2023-03/apps/linux-rhel8-zen/gcc-8.5.0/libjpeg-9e-rq7ceaj/lib:$LD_LIBRARY_PATH\nto be noticed by the install"
  },
  {
    "objectID": "materials/lectures/6-nn/slides.html#hugging-face-models",
    "href": "materials/lectures/6-nn/slides.html#hugging-face-models",
    "title": "LLMs and torch",
    "section": "Hugging Face Models",
    "text": "Hugging Face Models\n\nInstall hfhub\nInstall tok: needs module load rust\n\nMore on Hugging Face Next Week …"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#memory-hierarchy",
    "href": "materials/lectures/2-hardware/slides.html#memory-hierarchy",
    "title": "Memory Hierarchy and Hardware",
    "section": "Memory Hierarchy",
    "text": "Memory Hierarchy\n\nFrom Brown U, CSCI 0300 (2022) CC-BY 4.0 License \\(\\qquad\\) ns = \\(10^{-9}\\)second \\(\\quad\\) ms = \\(10^{-3}\\)second"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#memory-hierarchy-1",
    "href": "materials/lectures/2-hardware/slides.html#memory-hierarchy-1",
    "title": "Memory Hierarchy and Hardware",
    "section": "Memory Hierarchy",
    "text": "Memory Hierarchy\n\nEach level serves as cache for the level below\n\nA data request checks each cache until satisfied\nNot finding an item generates a “cache miss”\n\nA cache miss triggers a memory page swap from level below\n\n\nLarge data sets do not fit in cache: contiguous memory access patterns speed up code"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#memory-hierarchy-2",
    "href": "materials/lectures/2-hardware/slides.html#memory-hierarchy-2",
    "title": "Memory Hierarchy and Hardware",
    "section": "Memory Hierarchy",
    "text": "Memory Hierarchy"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#titan",
    "href": "materials/lectures/2-hardware/slides.html#titan",
    "title": "Memory Hierarchy and Hardware",
    "section": "Titan",
    "text": "Titan\n27 PF\n2012-19"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#delta-2022",
    "href": "materials/lectures/2-hardware/slides.html#delta-2022",
    "title": "Memory Hierarchy and Hardware",
    "section": "Delta: (2022)",
    "text": "Delta: (2022)\n8 PF"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#delta-2022-1",
    "href": "materials/lectures/2-hardware/slides.html#delta-2022-1",
    "title": "Memory Hierarchy and Hardware",
    "section": "Delta: (2022)",
    "text": "Delta: (2022)\n8 PF"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#delta-2022-2",
    "href": "materials/lectures/2-hardware/slides.html#delta-2022-2",
    "title": "Memory Hierarchy and Hardware",
    "section": "Delta: (2022)",
    "text": "Delta: (2022)\n8 PF"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware",
    "href": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware",
    "title": "Memory Hierarchy and Hardware",
    "section": "Two (Three) Basic Concepts in Hardware",
    "text": "Two (Three) Basic Concepts in Hardware"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#flynns-taxonomy-1972",
    "href": "materials/lectures/2-hardware/slides.html#flynns-taxonomy-1972",
    "title": "Memory Hierarchy and Hardware",
    "section": "Flynn’s Taxonomy (1972)",
    "text": "Flynn’s Taxonomy (1972)\n\nSingle Instruction, Single Data (SISD): scalar processor, serial program (old-style CPU)\nSingle Instruction, Multiple Data (SIMD): vector processor, array processor (old-style GPU)\nMultiple Instruction, Multiple Data (MIMD): multiple cores in a single processor, multiple processors in a single computer, and multiple computers in a cluster (today’s CPU and GPU, but each still leans to legacy)\nMultiple Instruction, Single Data (MISD): is not used much"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#three-processor-technology-streams",
    "href": "materials/lectures/2-hardware/slides.html#three-processor-technology-streams",
    "title": "Memory Hierarchy and Hardware",
    "section": "Three Processor Technology Streams",
    "text": "Three Processor Technology Streams\nComputers:\n- Fast and general computing\n\nGraphics:\n- Lots of pixels with same independent operations\n- Highly parallel and simple processing\n\nCellphones:\n- Small devices\n- Low power operation\n\nAll three have matured into large cluster deployment"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#shared-memory-hardware",
    "href": "materials/lectures/2-hardware/slides.html#shared-memory-hardware",
    "title": "Memory Hierarchy and Hardware",
    "section": "Shared Memory Hardware",
    "text": "Shared Memory Hardware\nCPU - Computer technology stream\n- One or two chips, each with multiple cores\n- Current high-end is 64 cores (e.g. AMD EPYC “Milan” on Delta)\n- Hosts operating system (OS)\nGPU - Graphics technology stream\n- Excellent SIMD throughput - Separate memory, so “offloading” co-processor concept\n- Lower power requirements per core, lots of slow cores\n- Does not host OS\nARM CPU-GPU - Cellphone technology stream\n- A hybrid unified memory architecture\n- Low power requirements\n- Hosts OS\nCPUs fast and versatile but limited parallelism\nGPUs slower and less versatile but extreme parallelism (4x+ cores)"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#uma-and-numa",
    "href": "materials/lectures/2-hardware/slides.html#uma-and-numa",
    "title": "Memory Hierarchy and Hardware",
    "section": "UMA and NUMA",
    "text": "UMA and NUMA\nUniform memory access\n\nA typical multicore CPU chip\n\nNon-Uniform memory access\n\nA larger collection of multicore chips with hardware/software enabled global memory access\nSome memory is closer than other memory\n\nMore complex cache strategies"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware-1",
    "href": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware-1",
    "title": "Memory Hierarchy and Hardware",
    "section": "Two (Three) Basic Concepts in Hardware",
    "text": "Two (Three) Basic Concepts in Hardware"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware-2",
    "href": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware-2",
    "title": "Memory Hierarchy and Hardware",
    "section": "Two (Three) Basic Concepts in Hardware",
    "text": "Two (Three) Basic Concepts in Hardware"
  },
  {
    "objectID": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware-3",
    "href": "materials/lectures/2-hardware/slides.html#two-three-basic-concepts-in-hardware-3",
    "title": "Memory Hierarchy and Hardware",
    "section": "Two (Three) Basic Concepts in Hardware",
    "text": "Two (Three) Basic Concepts in Hardware"
  },
  {
    "objectID": "materials/lectures/3-fork/slides.html#r-interfaces-to-low-level-native-tools",
    "href": "materials/lectures/3-fork/slides.html#r-interfaces-to-low-level-native-tools",
    "title": "Parallel via the Unix fork",
    "section": "R Interfaces to Low-Level Native Tools",
    "text": "R Interfaces to Low-Level Native Tools"
  },
  {
    "objectID": "materials/lectures/3-fork/slides.html#parallelmclapply",
    "href": "materials/lectures/3-fork/slides.html#parallelmclapply",
    "title": "Parallel via the Unix fork",
    "section": "parallel::mclapply()",
    "text": "parallel::mclapply()"
  },
  {
    "objectID": "materials/lectures/3-fork/slides.html#unix-fork",
    "href": "materials/lectures/3-fork/slides.html#unix-fork",
    "title": "Parallel via the Unix fork",
    "section": "Unix fork\n",
    "text": "Unix fork\n\nOperating system forks a new process with pointers to same memory: A memory-efficient parallelism on shared memory devices running Unix-like OS\n\nCopy-on-write: memory copied only if modified\n\nparallel package function mclapply() and friends\n\nUse for numerical sections only:\n\nAvoid GUI, I/O, and graphics sections (common device handles)\n\nGreat in a batch Rscript on Unix-like OS\n\n\nConvenient for data (read, not modified)\n\nConvenient for functional languages like R (parameters by value)\n\nManage or avoid nested parallelism:\n\nSet OpenBLAS cores\n\n\ndata.table automatically switches to single threaded mode upon fork\n\n\n\n\nA deeper discussion of fork memory (if you have interest) on YouTube by Chris Kanich (UIC)"
  },
  {
    "objectID": "materials/lectures/3-fork/slides.html#copy-on-write",
    "href": "materials/lectures/3-fork/slides.html#copy-on-write",
    "title": "Parallel via the Unix fork",
    "section": "Copy-on-write",
    "text": "Copy-on-write\n\nAll done with pointers\nMemory is in pages\nProcesses not aware of each other or other’s memory use\nOS is aware of memory use\n16 forks write = 16 copies of memory"
  },
  {
    "objectID": "materials/lectures/3-fork/slides.html#os-manages-core-affinity",
    "href": "materials/lectures/3-fork/slides.html#os-manages-core-affinity",
    "title": "Parallel via the Unix fork",
    "section": "OS manages core affinity",
    "text": "OS manages core affinity\n\n\n\\(\\qquad\\) OS tasks compete \\(\\qquad\\) Core switching occurs frequently"
  },
  {
    "objectID": "materials/lectures/3-fork/slides.html#code-first-with-lapply-then-switch-to-mclapply",
    "href": "materials/lectures/3-fork/slides.html#code-first-with-lapply-then-switch-to-mclapply",
    "title": "Parallel via the Unix fork",
    "section": "Code first with lapply(), then switch to mclapply()\n",
    "text": "Code first with lapply(), then switch to mclapply()\n\nDrop-in replacements (almost) for lapply(), mapply(), and Map()\n\nmclapply(X, FUN, ...,\n         mc.preschedule = TRUE, mc.set.seed = TRUE,\n         mc.silent = FALSE, mc.cores = getOption(\"mc.cores\", 2L),\n         mc.cleanup = TRUE, mc.allow.recursive = TRUE, affinity.list = NULL)\n\nmcmapply(FUN, ...,\n         MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE,\n         mc.preschedule = TRUE, mc.set.seed = TRUE,\n         mc.silent = FALSE, mc.cores = getOption(\"mc.cores\", 2L),\n         mc.cleanup = TRUE, affinity.list = NULL) \n\nmcMap(f, ...)\n\nDebugging easier with lapply()"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#four-levels-of-difficulty-in-data-science-modeling",
    "href": "materials/lectures/5-sampling/sampling.html#four-levels-of-difficulty-in-data-science-modeling",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Four Levels of Difficulty in Data Science Modeling",
    "text": "Four Levels of Difficulty in Data Science Modeling\n\n\nPrediction (easiest)\nEstimation (adds uncertainty: bias and variability)\n\nconsider the bootstrap for uncertainty (resampling and repeated prediction)\n\n\nAttribution (discerns contributors to system variability)\n\ncombinatorial difficutly: does adding a variable improve prediction and reduce uncertainty (order matters)\n\n\nCausality (adds control of system)\n\nconditional combinatorics or experiments (subsets matter)\n\n\n\n\n\nFrom a practical perspective, control or causality is usually the goal!\n\n\n\n2, 3, and 4 involve independent repetition of prediction, with opportunities for parallel computation!\n\n\n\nMotivation: high-level, not in the weeds\n\ntry to put your methodologies in this context\n\nask if it does not fit\n\n\n\ntook 10+ years to feel confident\n\nattribution to variability (not nec cause)\n\nsampling columns\n\n\nFor a deeper understanding see Efron, B. (2020). Prediction, Estimation, and Attribution. JASA and other recent literature."
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#attribution-why-combinatorial",
    "href": "materials/lectures/5-sampling/sampling.html#attribution-why-combinatorial",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Attribution: why combinatorial?",
    "text": "Attribution: why combinatorial?\n\n\n\nSuppose we have \\(n\\) observations on \\(p\\) predictors:\n\n\\(y = f(x_1, x_2, \\ldots, x_p) + \\epsilon \\qquad\\) Model\n\\(\\widehat{y} = \\widehat{f}(x_1, x_2, \\ldots, x_p) \\quad \\qquad\\) Model fit\n\\(\\frac{1}{n} || y - \\widehat{y} ||_2^2 = \\frac{1}{n} || y - \\widehat{f} ||_2^2 \\qquad\\) MSE Loss\n\n\nLet \\(S\\) be a subset of \\(\\{1, 2, \\ldots, p\\}\\).\nLet \\(\\widehat{f}_S\\) be a model fit using a subset \\(S\\) of the predictors and \\(\\mbox{MSE}_{S}\\) its Loss.\n\nFor two subsets of predictors, \\(S_1 \\neq S_2\\) where \\(k \\notin S_1\\), \\(k \\notin S_2\\),\n\nin general, \\(\\mbox{MSE}_{\\{S_1\\}} - \\mbox{MSE}_{\\{S_1, k\\}} \\neq \\mbox{MSE}_{\\{S_2\\}} - \\mbox{MSE}_{\\{S_2, k\\}}\\).\n\n\n\n\n\n\nThe contribution of \\(x_k\\) to reducing MSE Loss depends on what else is in the model!\nHence combinatorial complexity of attribution!\n\n\nModel: regression, lin or nonlin, nn, rand forest, boosting, bagging\nAssumptions on \\(f\\) and \\(x\\) can make this easier, e.g. lasso and oracle methods"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#methods-that-use-repeated-prediction",
    "href": "materials/lectures/5-sampling/sampling.html#methods-that-use-repeated-prediction",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Methods that Use Repeated Prediction",
    "text": "Methods that Use Repeated Prediction\n\n\n\nBootstrap: a tool for uncertainty quantification\n\nResample\n\n\n\nBagging: a tool for reducing variance\n\nAverage many simple models\n\n\n\nBoosting: a greedy method of growing a model\n\nRepeatedly model previous model error (Sequential!)\n\n\n\nCrossvalidation: model performance assessment tool, hyperparameter optimization\n\nEstimate out-of-sample Loss\n\n\n\n\n\nlasso aims at attribution (variable selection) assumptions on \\(f\\) and crossvalidation\n\nHastie, Tibshirani, and Friedman (2009) The Elements of Statistical Learning, Second Edition, (2009)"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#bootstrap---uncertainty-quantification",
    "href": "materials/lectures/5-sampling/sampling.html#bootstrap---uncertainty-quantification",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Bootstrap - uncertainty quantification",
    "text": "Bootstrap - uncertainty quantification\nResample data with replacement and repeat estimation/prediction\n\n\nData: \\({\\mathbf Z} = (z_1,z_2,\\ldots,z_N)\\), where \\(z_i = (y_i,x_i)\\)\nModel: Let \\(\\mu({\\mathbf Z})\\) be an estimated/predicted quantity from the data\nSample with replacement \\(B\\) sets of size \\(N\\) from data\nCompute \\(\\mu()\\) from each of the reseampled sets \\(\\{\\widehat{\\mu({\\mathbf Z^{*1}})}, \\widehat{\\mu({\\mathbf Z^{*2}})},\\ldots, \\widehat{\\mu({\\mathbf Z^{*B}})}\\}\\)\nProduces a sample from the estimator/predictor distribution\n\nMany characterizations are now possible:\n\nmean, variance, median, quantiles, density estimate, etc.\n\n\n\n\n\n\nParallelize over the \\(B\\) sets (more parallelism may exist within \\(\\mu(\\cdot)\\))\n\n\na weight vector (0, 1, 2, … )"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#bagging---variance-reduction",
    "href": "materials/lectures/5-sampling/sampling.html#bagging---variance-reduction",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Bagging - variance reduction",
    "text": "Bagging - variance reduction\nbootstrap aggregation\nSimple modelswith low bias and high variance on resampled data\nGeneralized by random forest to sampling subsets of predictors\n\n\nRegression: \\(\\widehat{\\mu({\\mathbf Z})} = \\frac{1}{B}\\sum_{b=1}^B \\widehat{\\mu({\\mathbf Z^{*b}})}\\) (bootstrap sample mean)\nClassification: Majority vote\nBagging reduces variance to give a low bias and low variance estimate\n\n\n\n\nParallelize over \\(B\\) (more parallelism may exist within \\(\\mu(\\cdot)\\))"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#random-forest",
    "href": "materials/lectures/5-sampling/sampling.html#random-forest",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Random Forest",
    "text": "Random Forest\n\n\n\nFor b = 1 to B:\n\n\nDraw a bootstrap sample \\({\\mathbf Z^∗}\\) from the training data\n\n\nGrow a random-forest tree \\(T_b\\) on \\(\\mathbf Z^*\\) until \\(n_{min}\\) nodes achieved:\n\n\nSelect \\(m &lt; p\\) variables at random\n\n\nPick the best variable/split-point among the \\(m\\)\n\n\nSplit the node into two daughter nodes\n\n\n\n\n\n\nOutput the ensemble of trees \\(\\{T_b\\}^B_1\\)\n\n\n\n\nRegression at \\(x\\): average the predictions\nof trees in the ensemble \\(\\frac{1}{B}\\sum_{b=1}^B \\widehat{T}_b(x)\\)\nClassification of \\(x\\):\nmajority vote class prediction \\(\\{\\widehat{C}_b(x)\\}^B_1\\)\n\n\n\npromotes diversity in trees\n\na mathematical proof that diversity is an optimization strategy\n\nprevents dominance by few strong predictors\n\n\n\nAlgorithm 15.1 in Hastie, et al. (2009). The Elements of Statistical Learning,\nSecond Edition"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#k-fold-crossvalidation",
    "href": "materials/lectures/5-sampling/sampling.html#k-fold-crossvalidation",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "K-fold Crossvalidation",
    "text": "K-fold Crossvalidation\n\n\nLet \\(y \\approx f(x, \\beta, \\alpha)\\) be a model and \\(\\widehat{y} = f(x, \\widehat{\\beta}, \\alpha)\\) be the model prediction of \\(y\\)\n\n\n\\((y, x)\\) are data, \\(\\beta\\) are model parameters, and \\(\\alpha\\) are model hyperparameters\n\nFor random forest: \\(\\beta\\) are split-points, \\(\\alpha\\) are \\(n_\\min\\), \\(m\\), nodesize, +\n\n\nFor a fixed \\(\\alpha\\), \\(\\widehat{\\beta}\\) minimizes a loss function \\(L(y, \\widehat{y} )\\)\n\n\n\nRandomly divide \\(N\\) training data points into \\(k\\) roughly equal folds (shuffle and split)\nLet \\(\\widehat{y_{-i}(\\alpha)} = f(x_{-i}, \\widehat{\\beta}, \\alpha)\\) be the prediction when fold \\(i\\) data is removed\nThen average loss for a choice of \\(\\alpha\\) is \\(\\frac{1}{N}\\sum_{i=1}^{N} L(y_i, \\widehat{y_{-i}}(\\alpha))\\)\n\nTaking \\(\\alpha\\) with minimum average loss, gives best expected generalization"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#pseudo-random-number-generators-rng",
    "href": "materials/lectures/5-sampling/sampling.html#pseudo-random-number-generators-rng",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Pseudo Random Number Generators (RNG)",
    "text": "Pseudo Random Number Generators (RNG)\n\n\n\nDefault RNG:\n\n\nL’Ecuyer-CMRG:\n\n\n\n\nHigh probability of independent streams\n\n\nGuaranteed independent streams\n\n\n\n\n\\(\\qquad\\)\n\n\nL’Ecuyer-CMRG stream is about 2^191 long\n\n\n\n\n\\(\\qquad\\)\n\n\nSubstreams jump 2^127 (about 1.7e38) for up to 2^64 (about 1.8e19) independent streams\n\n\n\n\n\n\\(\\qquad\\qquad\\qquad\\) Default RNG \\(\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\) L’Ecuyer-CMRG"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#example-random-forest-code",
    "href": "materials/lectures/5-sampling/sampling.html#example-random-forest-code",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Example Random forest Code",
    "text": "Example Random forest Code\nLetter recognition data ( \\(20\\,000 \\times 17\\) )\n\n\nTaken from: Parallel Statistical Computing with R: An Illustration on Two Architectures arXiv:1709.01195"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_serial.r",
    "href": "materials/lectures/5-sampling/sampling.html#rf_serial.r",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_serial.r",
    "text": "rf_serial.r\nsuppressMessages(library(randomForest))\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123)\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nrf.all = randomForest(lettr ~ ., train, ntree = 500, norm.votes = FALSE)\npred = predict(rf.all, test)\n\ncorrect = sum(pred == test$lettr)\ncat(\"Proportion Correct:\", correct/(n_test), \"\\n\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_serial.r-1",
    "href": "materials/lectures/5-sampling/sampling.html#rf_serial.r-1",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_serial.r",
    "text": "rf_serial.r\nsuppressMessages(library(randomForest))\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123)\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nrf.all = randomForest(lettr ~ ., train, ntree = 500, norm.votes = FALSE)\npred = predict(rf.all, test)\n\ncorrect = sum(pred == test$lettr)\ncat(\"Proportion Correct:\", correct/(n_test), \"\\n\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_mc.r",
    "href": "materials/lectures/5-sampling/sampling.html#rf_mc.r",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_mc.r",
    "text": "rf_mc.r\nlibrary(parallel)                                       #&lt;&lt;\nlibrary(randomForest)\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123, \"L'Ecuyer-CMRG\")                   #&lt;&lt;\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nnc = as.numeric(commandArgs(TRUE)[2])                    #&lt;&lt;\nntree = lapply(splitIndices(500, nc), length)            #&lt;&lt;\nrf = function(x, train) randomForest(lettr ~ ., train, ntree=x, norm.votes = FALSE) #&lt;&lt;\nrf.out = mclapply(ntree, rf, train = train, mc.cores = nc)      #&lt;&lt;\nrf.all = do.call(combine, rf.out)                        #&lt;&lt;\n\ncrows = splitIndices(nrow(test), nc)                     #&lt;&lt;\nrfp = function(x) as.vector(predict(rf.all, test[x, ]))  #&lt;&lt;\ncpred = mclapply(crows, rfp, mc.cores = nc)              #&lt;&lt;\npred = do.call(c, cpred)                                 #&lt;&lt;\n\ncorrect &lt;- sum(pred == test$lettr)\ncat(\"Proportion Correct:\", correct/(n_test), \"\\n\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_mc.r-1",
    "href": "materials/lectures/5-sampling/sampling.html#rf_mc.r-1",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_mc.r",
    "text": "rf_mc.r\nlibrary(parallel)                                       #&lt;&lt;\nlibrary(randomForest)\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123, \"L'Ecuyer-CMRG\")                   #&lt;&lt;\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nnc = as.numeric(commandArgs(TRUE)[2])                    #&lt;&lt;\nntree = lapply(splitIndices(500, nc), length)            #&lt;&lt;\nrf = function(x, train) randomForest(lettr ~ ., train, ntree=x, norm.votes = FALSE) #&lt;&lt;\nrf.out = mclapply(ntree, rf, train = train, mc.cores = nc)      #&lt;&lt;\nrf.all = do.call(combine, rf.out)                        #&lt;&lt;\n\ncrows = splitIndices(nrow(test), nc)                     #&lt;&lt;\nrfp = function(x) as.vector(predict(rf.all, test[x, ]))  #&lt;&lt;\ncpred = mclapply(crows, rfp, mc.cores = nc)              #&lt;&lt;\npred = do.call(c, cpred)                                 #&lt;&lt;\n\ncorrect &lt;- sum(pred == test$lettr)\ncat(\"Proportion Correct:\", correct/(n_test), \"\\n\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_mc.sh",
    "href": "materials/lectures/5-sampling/sampling.html#rf_mc.sh",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_mc.sh",
    "text": "rf_mc.sh\n#!/bin/bash\n#SBATCH --job-name utk\n#SBATCH --account=bckj-delta-cpu\n#SBATCH --partition=cpu\n#SBATCH --mem=20g\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=16\n#SBATCH --time 00:10:00\n#SBATCH -e ./utk.e\n#SBATCH -o ./utk.o\n\npwd\n\nmodule load r\nmodule list\n\ntime Rscript rf_serial.R\ntime Rscript rf_mc.R --args 1\ntime Rscript rf_mc.R --args 2\ntime Rscript rf_mc.R --args 4\ntime Rscript rf_mc.R --args 8\ntime Rscript rf_mc.R --args 16"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#crossvalidation-example",
    "href": "materials/lectures/5-sampling/sampling.html#crossvalidation-example",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "Crossvalidation Example",
    "text": "Crossvalidation Example\n\nRandom forest:\n\\(m\\), the number of predictors sampled \nThe defaults are:\n\n\\(m = p/3\\)\n\\(\\ldots\\)"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_cv_serial.r",
    "href": "materials/lectures/5-sampling/sampling.html#rf_cv_serial.r",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_cv_serial.R",
    "text": "rf_cv_serial.R\nlibrary(randomForest)\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123)\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nntree = 300\nnfolds = 10\nmtry_val = 1:(ncol(train) - 1)\nfolds = sample( rep_len(1:nfolds, nrow(train)), nrow(train) )\ncv_pars = expand.grid(mtry = mtry_val, f = 1:nfolds)\nfold_err = function(i, cv_pars, folds, train) {\n  mtry = cv_pars[i, \"mtry\"]\n  fold = (folds == cv_pars[i, \"f\"])\n  rf.all = randomForest(lettr ~ ., train[!fold, ], ntree = ntree, mtry = mtry, norm.votes = FALSE)\n  pred = predict(rf.all, train[fold, ])\n  sum(pred != train$lettr[fold])\n}\n\ncv_err = lapply(1:nrow(cv_pars), fold_err, cv_pars, folds = folds, train = train)\ncv_err = tapply(unlist(cv_err), cv_pars[, \"mtry\"], sum)\n\npdf(paste0(\"rf_cv_mc0.pdf\"))\n  plot(mtry_val, cv_err/(n - n_test))\ndev.off()\n\nrf.all = randomForest(lettr ~ ., train, ntree = ntree)\npred = predict(rf.all, test)\ncorrect = sum(pred == test$lettr)\n\nmtry = mtry_val[which.min(cv_err)]\nrf.all = randomForest(lettr ~ ., train, ntree = ntree, mtry = mtry)\npred_cv = predict(rf.all, test)\ncorrect_cv = sum(pred_cv == test$lettr)\ncat(\"Proportion Correct: \", correct/n_test, \"(mtry = \", floor((ncol(test) - 1)/3),\n    \") with cv:\", correct_cv/n_test, \"(mtry = \", mtry, \")\\n\", sep = \"\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_cv_serial.r-1",
    "href": "materials/lectures/5-sampling/sampling.html#rf_cv_serial.r-1",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_cv_serial.R",
    "text": "rf_cv_serial.R\nlibrary(randomForest)\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123)\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nntree = 300\nnfolds = 10\nmtry_val = 1:(ncol(train) - 1)\nfolds = sample( rep_len(1:nfolds, nrow(train)), nrow(train) )\ncv_pars = expand.grid(mtry = mtry_val, f = 1:nfolds)\nfold_err = function(i, cv_pars, folds, train) {\n  mtry = cv_pars[i, \"mtry\"]\n  fold = (folds == cv_pars[i, \"f\"])\n  rf.all = randomForest(lettr ~ ., train[!fold, ], ntree = ntree, mtry = mtry, norm.votes = FALSE)\n  pred = predict(rf.all, train[fold, ])\n  sum(pred != train$lettr[fold])\n}\n\ncv_err = lapply(1:nrow(cv_pars), fold_err, cv_pars, folds = folds, train = train)\ncv_err = tapply(unlist(cv_err), cv_pars[, \"mtry\"], sum)\n\npdf(paste0(\"rf_cv_mc0.pdf\"))\n  plot(mtry_val, cv_err/(n - n_test))\ndev.off()\n\nrf.all = randomForest(lettr ~ ., train, ntree = ntree)\npred = predict(rf.all, test)\ncorrect = sum(pred == test$lettr)\n\nmtry = mtry_val[which.min(cv_err)]\nrf.all = randomForest(lettr ~ ., train, ntree = ntree, mtry = mtry)\npred_cv = predict(rf.all, test)\ncorrect_cv = sum(pred_cv == test$lettr)\ncat(\"Proportion Correct: \", correct/n_test, \"(mtry = \", floor((ncol(test) - 1)/3),\n    \") with cv:\", correct_cv/n_test, \"(mtry = \", mtry, \")\\n\", sep = \"\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_cv_mc.r",
    "href": "materials/lectures/5-sampling/sampling.html#rf_cv_mc.r",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_cv_mc.R",
    "text": "rf_cv_mc.R\nlibrary(randomForest)\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123, \"L'Ecuyer-CMRG\")\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nntree = 200\nnfolds = 10\nmtry_val = 1:(ncol(train) - 1)\nfolds = sample( rep_len(1:nfolds, nrow(train)), nrow(train) )\ncv_pars = expand.grid(mtry = mtry_val, f = 1:nfolds)\nfold_err = function(i, cv_pars, folds, train) {\n  mtry = cv_pars[i, \"mtry\"]\n  fold = (folds == cv_pars[i, \"f\"])\n  rf.all = randomForest(lettr ~ ., train[!fold, ], ntree = ntree,\n                        mtry = mtry, norm.votes = FALSE)\n  pred = predict(rf.all, train[fold, ])\n  sum(pred != train$lettr[fold])\n}\n\nnc = as.numeric(commandArgs(TRUE)[2])\ncv_err = parallel::mclapply(1:nrow(cv_pars), fold_err, cv_pars, folds = folds, train = train, mc.cores = nc)\nerr = tapply(unlist(cv_err), cv_pars[, \"mtry\"], sum)\npdf(paste0(\"rf_cv_mc\", nc, \".pdf\"))\n  plot(mtry_val, err/(n - n_test))\ndev.off()\n\nrf.all = randomForest(lettr ~ ., train, ntree = ntree)\npred = predict(rf.all, test)\ncorrect = sum(pred == test$lettr)\n\nmtry = mtry_val[which.min(err)]\nrf.all = randomForest(lettr ~ ., train, ntree = ntree, mtry = mtry)\npred_cv = predict(rf.all, test)\ncorrect_cv = sum(pred_cv == test$lettr)\ncat(\"Proportion Correct: \", correct/n_test, \"(mtry = \", floor((ncol(test) - 1)/3),\n    \") with cv:\", correct_cv/n_test, \"(mtry = \", mtry, \")\\n\", sep = \"\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#rf_cv_mc.r-1",
    "href": "materials/lectures/5-sampling/sampling.html#rf_cv_mc.r-1",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "rf_cv_mc.R",
    "text": "rf_cv_mc.R\nlibrary(randomForest)\ndata(LetterRecognition, package = \"mlbench\")\nset.seed(seed = 123, \"L'Ecuyer-CMRG\")\n\nn = nrow(LetterRecognition)\nn_test = floor(0.2 * n)\ni_test = sample.int(n, n_test)\ntrain = LetterRecognition[-i_test, ]\ntest = LetterRecognition[i_test, ]\n\nntree = 200\nnfolds = 10\nmtry_val = 1:(ncol(train) - 1)\nfolds = sample( rep_len(1:nfolds, nrow(train)), nrow(train) )\ncv_pars = expand.grid(mtry = mtry_val, f = 1:nfolds)\nfold_err = function(i, cv_pars, folds, train) {\n  mtry = cv_pars[i, \"mtry\"]\n  fold = (folds == cv_pars[i, \"f\"])\n  rf.all = randomForest(lettr ~ ., train[!fold, ], ntree = ntree,\n                        mtry = mtry, norm.votes = FALSE)\n  pred = predict(rf.all, train[fold, ])\n  sum(pred != train$lettr[fold])\n}\n\nnc = as.numeric(commandArgs(TRUE)[2])\ncv_err = parallel::mclapply(1:nrow(cv_pars), fold_err, cv_pars, folds = folds, train = train, mc.cores = nc)\nerr = tapply(unlist(cv_err), cv_pars[, \"mtry\"], sum)\npdf(paste0(\"rf_cv_mc\", nc, \".pdf\"))\n  plot(mtry_val, err/(n - n_test))\ndev.off()\n\nrf.all = randomForest(lettr ~ ., train, ntree = ntree)\npred = predict(rf.all, test)\ncorrect = sum(pred == test$lettr)\n\nmtry = mtry_val[which.min(err)]\nrf.all = randomForest(lettr ~ ., train, ntree = ntree, mtry = mtry)\npred_cv = predict(rf.all, test)\ncorrect_cv = sum(pred_cv == test$lettr)\ncat(\"Proportion Correct: \", correct/n_test, \"(mtry = \", floor((ncol(test) - 1)/3),\n    \") with cv:\", correct_cv/n_test, \"(mtry = \", mtry, \")\\n\", sep = \"\")"
  },
  {
    "objectID": "materials/lectures/5-sampling/sampling.html#r-interfaces-to-low-level-native-tools",
    "href": "materials/lectures/5-sampling/sampling.html#r-interfaces-to-low-level-native-tools",
    "title": "ML/Stat Methods Relying on Sampling: case for parallel computing",
    "section": "R Interfaces to Low-Level Native Tools",
    "text": "R Interfaces to Low-Level Native Tools\n\nwe begin with paralel’s multicore parts\ncontinue with Foreign language via libraries (OpenBLAS, nvBLAS)\ngo to SPMD MPI with collectives\nreverse of history - because we are used to a laptop\nDistributed - some things are recomputed rather than communicated"
  },
  {
    "objectID": "materials/lectures/1-welcome/slides.html#today",
    "href": "materials/lectures/1-welcome/slides.html#today",
    "title": "Welcome to HPC for Data Science in R!  BZAN 583",
    "section": "Today:",
    "text": "Today:\n\n\nWelcome!!\nthx for signing up for the class!\n\n\n\nWelcome!! Overview & Syllabus\nAssign groups of 3-4\n\nSurvey and expectations\n\nStart setting up accounts (GitHub, Delta cluster) -Assignment\n\nIntroduction to workflow: RStudio to remote cluster via GitHub"
  },
  {
    "objectID": "materials/lectures/1-welcome/slides.html#random-group-assignment",
    "href": "materials/lectures/1-welcome/slides.html#random-group-assignment",
    "title": "Welcome to HPC for Data Science in R!  BZAN 583",
    "section": "Random Group Assignment",
    "text": "Random Group Assignment\nClass divided into groups of 3 to 4\n\nAssigned randomly via Canvas\n\nSwapping is allowed (after class today) if all members of concerned groups agree\n\nHandout\n\nGroup assignments on the front\n\nSurvey on the back\n\nFill out now\n\nWill collect at end of class today"
  },
  {
    "objectID": "materials/lectures/1-welcome/slides.html#survey",
    "href": "materials/lectures/1-welcome/slides.html#survey",
    "title": "Welcome to HPC for Data Science in R!  BZAN 583",
    "section": "Survey",
    "text": "Survey\n\n\nYour laptop: Unix, Mac, Windows\n\nR experience: used in a class, used in two classes, x years, wrote an R package, live and breathe R\n\nUse RStudio? usually, all the time, reluctantly, prefer Jupyter, prefer Visual Studio\nOther programming languages: Python, C, C++, Go, …\n\nUse git? what’s that?, heard of it, tried it once, use it often, I live on it\nR parallel package? what’s that?, heard of it, tried it once, didn’t help me, got great results, keeps all the cores busy on my laptop"
  },
  {
    "objectID": "materials/lectures/1-welcome/slides.html#survey-continued",
    "href": "materials/lectures/1-welcome/slides.html#survey-continued",
    "title": "Welcome to HPC for Data Science in R!  BZAN 583",
    "section": "Survey … continued",
    "text": "Survey … continued\n\n\nUnix experince (Linux, MacOS terminal): never used it, tried it once, know a few commands, I write shell scripts\nHPC Cluster? I know they exist, saw one, logged in once, used one for a project, keep one in my basement\nCloud computing? is it real?, heard about it, MS Azure, AWS, UTK cluster VM, Other\nMPI? what’s that?, heard of it, used a code that used it, wrote some code\nTorch or LLM experience? a few words\nYour expectations for this class: play with HPC, Build an HPC Analytics Startup Co\n\n\n\n\nWorkload: How many classes are you taking at the moment?\n\nShow of hands: zero, one, two, three, …\n\nLanguages: FORTRAN, Cobol, machine code, assembler code, APL, Snobol, JCL, SAS, C, unix shell, S, R\n\nDocument preparation: nroff, ps, pdf, LaTeX, Markdown, .Rmd, Quarto .qmd\nSystems: IBM, DEC, Honeywell, VAX, PC, Intel, Mac, HPE (formerly Cray),"
  },
  {
    "objectID": "materials/lectures/1-welcome/slides.html#setting-up-accounts-on-github-on-delta",
    "href": "materials/lectures/1-welcome/slides.html#setting-up-accounts-on-github-on-delta",
    "title": "Welcome to HPC for Data Science in R!  BZAN 583",
    "section": "Setting up Accounts on GitHub, on Delta",
    "text": "Setting up Accounts on GitHub, on Delta\nAssignment 1"
  },
  {
    "objectID": "materials/lectures/1-welcome/slides.html#workflow-for-remote-computing-in-r",
    "href": "materials/lectures/1-welcome/slides.html#workflow-for-remote-computing-in-r",
    "title": "Welcome to HPC for Data Science in R!  BZAN 583",
    "section": "Workflow for Remote Computing in R",
    "text": "Workflow for Remote Computing in R"
  },
  {
    "objectID": "materials/assignment/5-resampling/index.html",
    "href": "materials/assignment/5-resampling/index.html",
    "title": "Assignment 5: Resampling",
    "section": "",
    "text": "Do a scaling study of one resampling method (bootstrap, bagging, random forest, crossvalidation) on your group’s data. This can become part of your project, but if you prefer to use other data, use one month of the yellow taxi data from any year on Delta. If in doubt on what to choose or have other ideas, please consult the instructor.\nResult1: The serial code and a paragraph on what is your estimator/prediction and its resampling accomplishing.\nResult2: The parallel code.\nResult3: Delta compute node times for\n- serial code\n- 1-core mclapply\n- 2-core mclapply\n- 4-core mclapply\n- 8-core mclapply\n- 16-core mclapply"
  },
  {
    "objectID": "materials/assignment/4-data/index.html",
    "href": "materials/assignment/4-data/index.html",
    "title": "Assignment 4: Project Data",
    "section": "",
    "text": "Each Team has chosen a data set for their project. This assignment is to jump start the projects so that each team has a working and fast-readable data set on Delta by the end of this week.\nThere are team directories Team1 through Team6 on the Delta /project/bckj file system.\nResult1: Download your data set to your /projects/bckj/Team# directory, into a subdirectory to keep things organized.\nResult2: Convert your data set into a parquet format, partitioned by a well chosen variable, using the arrow package and write it to another subdirectory under /projects/bckj/Team#. Good partition size is somewhere between 100 and 500 MB.\nResult3: Get the sizes of each of the two directories with du -hs &lt;directory&gt; and submit the text result of these commands and their output.\nThe only output to hand in to Canvas are the four lines of Result 3. Of course, the data directories on Delta need to contain the data reported by Result3.\nNew: Result2 is now optional and Result3 requirement is now only two lines.\nHowever for your project (not required for this assignment), you should consider how long it takes to read your data into R and strategies for reducing it. One option is to break up a CSV file into smaller pieces if too big. Timing a single file read versus parallel and smaller reads would decide which is better. Another option is to consider HDF5 format via the rhdf5 package. If you have many image files, consider organizing them in several directories for parallel treatment across the directories."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "HPC for Data Science in R - BZAN 583",
    "section": "",
    "text": "These “HPCinR Course Materials” by George Ostrouchov are licensed under CC-BY-4.0"
  },
  {
    "objectID": "materials/assignment/1-workflow/index.html",
    "href": "materials/assignment/1-workflow/index.html",
    "title": "Week 1 Assignment: Setup, Installations, and first Unix Steps",
    "section": "",
    "text": "Current version of R from CRAN\n\nCurrent version of RStudio Desktop from posit\n\nCurrent git version as described in happywithgit.com"
  },
  {
    "objectID": "materials/assignment/1-workflow/index.html#install-on-your-laptop",
    "href": "materials/assignment/1-workflow/index.html#install-on-your-laptop",
    "title": "Week 1 Assignment: Setup, Installations, and first Unix Steps",
    "section": "",
    "text": "Current version of R from CRAN\n\nCurrent version of RStudio Desktop from posit\n\nCurrent git version as described in happywithgit.com"
  },
  {
    "objectID": "materials/assignment/1-workflow/index.html#create-a-personal-account-on-github.com",
    "href": "materials/assignment/1-workflow/index.html#create-a-personal-account-on-github.com",
    "title": "Week 1 Assignment: Setup, Installations, and first Unix Steps",
    "section": "Create a personal account on github.com\n",
    "text": "Create a personal account on github.com\n\n\nGo to github.com and Sign up for a GitHub free personal account\n\nIf you already have one, you can continue to use it for this class"
  },
  {
    "objectID": "materials/assignment/1-workflow/index.html#setup-your-ncsa-delta-account",
    "href": "materials/assignment/1-workflow/index.html#setup-your-ncsa-delta-account",
    "title": "Week 1 Assignment: Setup, Installations, and first Unix Steps",
    "section": "Setup your NCSA Delta Account",
    "text": "Setup your NCSA Delta Account\n\n\nTo set up an account on Delta, first register at the ACCESS link. Then email the instructor your user id from the registration. The instructor can add you the the Delta resource allocation at this point.\nIt takes a day or two to receive access to login.delta.ncsa.illinois.edu. Until then, you may see the message “too many authentication failures.” When you have access, login via ssh to &lt;your-delta-username&gt;@login.delta.ncsa.illinois.edu. This will request your password and ask whether to do a DUO-push to your mobile phone. Below is an example with your instructor’s username login from a Mac Terminal shell. Only the first line is an interaction with the Mac shell (% prompt) and the rest is coming from the Delta login node ($ prompt). \n\nost@GO ~ % ssh gostrouc@login.delta.ncsa.illinois.edu\n\nNCSA Delta System\n\nLogin with NCSA Kerberos + NCSA Duo multi-factor.\n\nDUO Documentation:  https://go.ncsa.illinois.edu/2fa\n\ngostrouc@login.delta.ncsa.illinois.edu's password: \n(gostrouc@login.delta.ncsa.illinois.edu) Duo two-factor login for gostrouc\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-1037\n\nPasscode or option (1-1): 1\nSuccess. Logging you in...\ndt-login02.delta.ncsa.illinois.edu (141.142.140.195)\n  OS: RedHat 8.8   HW: HPE   CPU: 128x    RAM: 252 GB\n\n      ΔΔΔΔΔ    ΔΔΔΔΔΔ   ΔΔ     ΔΔΔΔΔΔ   ΔΔ\n      ΔΔ  ΔΔ   ΔΔ       ΔΔ       ΔΔ    ΔΔΔΔ\n      ΔΔ  ΔΔ   ΔΔΔΔ     ΔΔ       ΔΔ   ΔΔ  ΔΔ\n      ΔΔ  ΔΔ   ΔΔ       ΔΔ       ΔΔ   ΔΔΔΔΔΔ\n      ΔΔΔΔΔ    ΔΔΔΔΔΔ   ΔΔΔΔΔΔ   ΔΔ   ΔΔ  ΔΔ\n\n  User Guide:  https://go.ncsa.illinois.edu/deltauserdoc\n\n\nLast failed login: Mon Mar 18 18:54:12 CDT 2024 from 73.121.22.186 on ssh:notty\nThere was 1 failed login attempt since the last successful login.\n[gostrouc@dt-login02 ~]$ \n\nDelta user assistance tells me to ignore the “1 failed login” message. If you still need login help, email help@ncsa.illinois.edu.\nTo log out, exit at the $ prompt.\nResult 1: Turn in a copy of your session running the following shell commands in the sequence given on a Delta login node. The intent here is to verify that you can successfully connect to a Delta login node and run a few basic Unix commands.\n\nhostname\ndate\nw\nwhoami\nls\npwd\nmkdir R\nls\nls -a\nls -la\nls -laF --color\ncd R\npwd\ncd\npwd\nmodule load r\nmodule list\nRscript -e \"sessionInfo()\"\n\nResult 2: Use the man command to get a description of what each command does. Use man man to get an explanation of the man command. Turn in a short one-sentence description for each unique command.\nResult 3: From GitHub … your GitHub user id. So I can invite you to the BZAN-583 team in the HPCinR organization."
  },
  {
    "objectID": "materials/assignment/3-project/index.html",
    "href": "materials/assignment/3-project/index.html",
    "title": "Assignment 3: Project Proposals",
    "section": "",
    "text": "This assignment is the first to be completed, turned in, and graded by groups.\nThe proposal should include (1) a large data set (2) a description of what will be studied, tested, or predicted, (3) a description of the ML methodologies expected to be used.\nGroups are encouraged to meet on their own one or more times before discussing the proposal with the instructor.\nEach group will set up a 30 minute meeting time with the Instructor during this week to discuss their proposed project.\nVarious times to meet are available on Tuesday, Wednesday, Thursday, and Friday of this week. Please email go@tennessee.edu and suggest two possible time slots.\nAn approved proposal gets the full 12 points for each group member."
  },
  {
    "objectID": "materials/assignment/2-faster/index.html",
    "href": "materials/assignment/2-faster/index.html",
    "title": "Week 2 Assignment: Faster",
    "section": "",
    "text": "Find some R code that YOU put together for a prior assignment or project in another class over the last year or two, profile it, and make changes that make it at least 2x faster. This can be a single function from a larger code but it should not be just trivial. Roughly 10 to 20 lines of your R code that originally takes more than, say, 30 seconds to run.\nThis is your video game! When you get things 2x faster, you just beat the game and you move up a level!\nTwo files profile.sh and profile.R have been added to the Template GitHub repository HPCinR/BZAN_583_code. You will need to have a copy of this repository in your GitHub account and clone the copy to your laptop and to login.delta.ncsa.illinois.edu. How you proceed, depends on whether you already have the ealier copy of the Template in your GitHub account.\nNo earlier copy: (1) Copy HPCinR/BZAN_583_code to your GitHub account with “Use this template” green button. (2) Clone your copy to your laptop RStudio as New Project. (3) Clone the same to your login.delta.ncsa.illinois.edu account with git clone ....\nHave earlier copy: Easiest (and recommended) is to just copy and paste the two new files into your RStudio project of the earlier copy. Then commit and push to GitHub. At this point you are ready to git pull them on Delta. An alternative is to do a proper git update with these Updating Templates instructions.\nOnce you have the two files profile.sh and profile.R on Delta, you can submit the batch job to run the code with\n\nsbatch profile.sh\n\nReplace the prcomp() function entire line in profile.R with your code to profile. You do this in your laptop RStudio session, commit and push to GitHub and pull on Delta. Then you submit it again as above. Your output will be in utk.e and utk.o. Rinse and repeat until you have faster code.\nSubmit the following four text files to Canvas as your solution:\nResult 1: Two or three sentences that describe where the code was used originally and its purpose. The intent is documentation that this is YOUR R code and to give me some understanding of the code.\nResult 2: The code, its summary profile with top components, and the system.time() of the code “before”.\nResult 3: The code, its summary profile with top components, and the system.time() of the code “after”.\nResult 4: Two or three sentences that explain your changes and why things got faster.\nInstalling Packages on Delta: You will need to install any packages that your code uses on Delta. Packages can be installed on a Delta login node from an interactive R session. Login to Delta, then start an interactive R session at the $ shell prompt:\n\nmodule load r\nR\n\nAt this point you are in an R session. For example, to install dplyr\n\ninstall.packages(\"dplyr\", repo = \"https://cloud.r-project.org/\")\n\nand similarly for other packages. To exit R, use q()."
  },
  {
    "objectID": "materials/lectures/1-welcome/index.html",
    "href": "materials/lectures/1-welcome/index.html",
    "title": "Welcome to HPC for Data Science",
    "section": "",
    "text": "Welcome to HPC for Data Science in R. This class …\nFull Screen"
  },
  {
    "objectID": "materials/lectures/5-sampling/index.html",
    "href": "materials/lectures/5-sampling/index.html",
    "title": "Parallelizing Methods Relying on Sampling",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "materials/lectures/3-fork/index.html",
    "href": "materials/lectures/3-fork/index.html",
    "title": "Th Unix fork",
    "section": "",
    "text": "Using the Unix fork from R via the parallel package\nFull Screen"
  },
  {
    "objectID": "materials/lectures/2-hardware/index.html",
    "href": "materials/lectures/2-hardware/index.html",
    "title": "Hardware and Memory Hierarchies",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "materials/lectures/6-nn/index.html",
    "href": "materials/lectures/6-nn/index.html",
    "title": "LLMs and NNs",
    "section": "",
    "text": "LLMs and torch\nFull Screen"
  },
  {
    "objectID": "materials/lectures/7-distributed/index.html",
    "href": "materials/lectures/7-distributed/index.html",
    "title": "Distributed Computing",
    "section": "",
    "text": "Distributed computing for multinode computation\nFull Screen"
  },
  {
    "objectID": "materials/lectures/8-presentations/index.html",
    "href": "materials/lectures/8-presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Lightning talks about projects\nFull Screen"
  },
  {
    "objectID": "materials/lectures/4-parallel/index.html",
    "href": "materials/lectures/4-parallel/index.html",
    "title": "Shared memory parallel computing",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "materials/lectures/3-software/index.html",
    "href": "materials/lectures/3-software/index.html",
    "title": "Welcome to HPC for Data Science",
    "section": "",
    "text": "Parallel software to make your R code faster. How to parallelize in Unix flavors.\nFull Screen"
  },
  {
    "objectID": "materials/lectures/1a-workflow/index.html",
    "href": "materials/lectures/1a-workflow/index.html",
    "title": "Workflow for Remote Computing with R",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "materials/lectures/6-blas/index.html",
    "href": "materials/lectures/6-blas/index.html",
    "title": "BLAS Libraries",
    "section": "",
    "text": "Basic Linear Algebra Software (BLAS) Libraries\nFull Screen"
  },
  {
    "objectID": "materials/lectures/4-data/index.html",
    "href": "materials/lectures/4-data/index.html",
    "title": "Data",
    "section": "",
    "text": "Getting and processing data\nFull Screen"
  },
  {
    "objectID": "materials/lectures/1b-setupdemo/index.html",
    "href": "materials/lectures/1b-setupdemo/index.html",
    "title": "Workflow for Remote Computing with R",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "materials/lectures/2-faster/index.html",
    "href": "materials/lectures/2-faster/index.html",
    "title": "Faster R Scripts",
    "section": "",
    "text": "This lecture is about strategies to make your R code faster without explicit parallelization.\nFull Screen\n\n&lt;!–\n* Reference and use the Rfast package\n* Companion doc: Taking R to its limits: 70+ tips (bad grammar but useful tips) * Mangalore tutorial, JSM tutorial, other? –&gt;"
  },
  {
    "objectID": "style.html",
    "href": "style.html",
    "title": "Style",
    "section": "",
    "text": "Google’s R Style Guide\nTidyverse Style Guide\n\n\nWhat the documentation says.\nHistory. Anyone use the APL language?\nDiscussion\nMore Discussion\nThere are pros and cons to each one. My personal choice is to use = because it is easier and it aligns with most other programming languages. Also, in places where it matters which one is used, performing an assignment is considered bad programming practice.\nWhile assigning a variable inside a function parameter list is considered bad practice, it is sometimes used temporarily while measuring execution time with system.time(). If &lt;- is used in an expression, adding system.time() works as expected.\nsystem.time(x &lt;- my_function())\nBut if using = for assignment, we need to add braces, { and }, or additional parentheses. Otherwise = is interpreted to set a value of a function parameter in system.time().\nsystem.time({x = my_function()})\nUsually, omitting the braces triggers an unknown parameter error, but if the assigned variable is a parameter in system.time() it can fail silently!\nEither is fine in this class but be aware of the difference and be consistent in using one or the other. This class will not consider programming style in evaluations."
  },
  {
    "objectID": "style.html#programming-style",
    "href": "style.html#programming-style",
    "title": "Style",
    "section": "",
    "text": "Google’s R Style Guide\nTidyverse Style Guide\n\n\nWhat the documentation says.\nHistory. Anyone use the APL language?\nDiscussion\nMore Discussion\nThere are pros and cons to each one. My personal choice is to use = because it is easier and it aligns with most other programming languages. Also, in places where it matters which one is used, performing an assignment is considered bad programming practice.\nWhile assigning a variable inside a function parameter list is considered bad practice, it is sometimes used temporarily while measuring execution time with system.time(). If &lt;- is used in an expression, adding system.time() works as expected.\nsystem.time(x &lt;- my_function())\nBut if using = for assignment, we need to add braces, { and }, or additional parentheses. Otherwise = is interpreted to set a value of a function parameter in system.time().\nsystem.time({x = my_function()})\nUsually, omitting the braces triggers an unknown parameter error, but if the assigned variable is a parameter in system.time() it can fail silently!\nEither is fine in this class but be aware of the difference and be consistent in using one or the other. This class will not consider programming style in evaluations."
  }
]