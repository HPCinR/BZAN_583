---
title: "Shared Memory Parallel Computing"
format: 
  revealjs:
    chalkboard: true
editor: source
---

## Measurement and terminology of parallel speedup  

![](/pics/fork/Slide2.png)

## Embarrassingly (Pleasingly) Parallel  

- $$t_p = \frac{t}{n} = t_n$$
- $$\mbox{Speedup:}\quad\frac{t}{t_p} = n$$
$t$ - Serial time  
$n$ - Number of chunks (or processes)  
$t_n$ - Single chunk time with $n$ chunks   
$t_p$ - Parallel time  

## Serial Section (Amdahl's Law) {background-image="/pics/fork/Slide3.png" background-size="90%" background-position="center" .smaller}  

$$\qquad$$  
$$t_p = t_s + t_n > t_s$$  
$$\mbox{Max Speedup:}\quad\lim_{n \to \infty}\frac{t}{t_p} = \frac{t}{t_s}$$   

$t$ - Serial time (**fixed**)  
$n$ - Number of chunks (or processes)  
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  

#### Strong Scaling: fixed work, increasing resources  

## Serial Section (Gustafson's Law) {background-image="/pics/fork/Slide3.png" background-size="90%" background-position="center" .smaller}  

$$\qquad$$  
$$t_p = t_s + t_n$$  
$$\mbox{Speedup:}\quad\frac{t}{t_p} = \frac{t_s + nt_n}{t_s + t_n} = O(n)$$   

$t$ - Serial time (**growing**: $t_{2n} = 2t_n$ )  
$n$ - Number of chunks (or processes)  
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  

#### Weak Scaling: increasing work, increasing resources  

::: notes
Weak: misnomer: the speedup is actually great
:::

## Parallel Overhead {background-image="/pics/fork/Slide4.png" background-size="90%" background-position="center" .smaller}

$$\qquad$$  $$\qquad$$  
$$t_p = t_s + t_n + t_o(n)$$  
$t$ - Serial time  
$n$ - Number of processes  
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  
$t_o(n)$ - Parallel overhead time  

## Replication of Serial Section (Distirbuted) {background-image="/pics/fork/Slide5.png" background-size="90%" background-position="center" .smaller}

$$\qquad$$  $$\qquad$$  $$\qquad$$  $$\qquad$$  

#### Suppose the serial section is computed on one rank and sent to all ranks. Then overhead is a function of $n$!

$t$ - Serial time  
$n$ - Number of processes   
$t_n$ - single chunk time with $n$ chunks  
$t_p$ - Parallel time  
$t_s$ - Serial section time  
$t_o(n)$ - Parallel overhead time  

#### Replication can reduce communication overhead due to communication

